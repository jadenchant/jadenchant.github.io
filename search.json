[
  {
    "objectID": "pages/waffle.html",
    "href": "pages/waffle.html",
    "title": "Waffle",
    "section": "",
    "text": "This section includes an application of the waffle package which is used to make waffle and pictograph charts in R. The pictograph portion was not working when I created this.",
    "crumbs": [
      "Waffle"
    ]
  },
  {
    "objectID": "pages/waffle.html#intro",
    "href": "pages/waffle.html#intro",
    "title": "Waffle",
    "section": "",
    "text": "This section includes an application of the waffle package which is used to make waffle and pictograph charts in R. The pictograph portion was not working when I created this.",
    "crumbs": [
      "Waffle"
    ]
  },
  {
    "objectID": "pages/waffle.html#variable-names-for-mn-economic-industry-dataset-2017",
    "href": "pages/waffle.html#variable-names-for-mn-economic-industry-dataset-2017",
    "title": "Waffle",
    "section": "Variable Names for MN Economic Industry Dataset 2017",
    "text": "Variable Names for MN Economic Industry Dataset 2017\nsector = NAICS economic sector estab = Number of establishments napcsdol = Sales, value of shipments, or revenue of NAPCS collection code ($1,000)\n44 Retail Trade 61 Educational Services 62 Health Care and Social Assistance 71 Arts, Entertainment and Recreation 72 Accommodation and Food Services 81 Personal and Laundry Services\n\nmn_eco_industry &lt;- clean_names(mn_eco_industry_raw) |&gt;\n  select(naics2017, naics2017_label, sector, estab, napcsdol) |&gt;\n  mutate(estab = as.numeric(estab)) |&gt;\n  filter(!is.na(estab)) |&gt;\n  mutate(napcsdol = as.numeric(napcsdol)) |&gt;\n  mutate(sector_label = case_when(\n           sector == 44 ~ \"Retail Trade\",\n           sector == 61 ~ \"Educational Services\",\n           sector == 62 ~ \"Health Care and Social Assistance\",\n           sector == 71 ~ \"Arts, Entertainment and Recreation\",\n           sector == 72 ~ \"Accommodation and Food Services\",\n           sector == 81 ~ \"Personal and Laundry Services\",\n           TRUE ~ as.character(sector)\n         ))",
    "crumbs": [
      "Waffle"
    ]
  },
  {
    "objectID": "pages/waffle.html#example-waffle-chart",
    "href": "pages/waffle.html#example-waffle-chart",
    "title": "Waffle",
    "section": "Example Waffle Chart",
    "text": "Example Waffle Chart\n\nexample &lt;- tibble(\n  parts = factor(rep(month.abb[1:3], 3), levels=month.abb[1:3]),\n  vals = c(10, 20, 30, 6, 14, 40, 30, 20, 10),\n)\n\nexample |&gt;\n  count(parts, wt = vals) |&gt;\n  ggplot(\n    aes(fill = parts, values = n)\n  ) +\n  geom_waffle(\n    n_rows = 20,\n    size = 0.33, \n    colour = \"white\",\n    flip = TRUE\n  ) +\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#a40000\", \"#169948\", \"#ae6056\"),\n    labels = c(\"Fruit\", \"Salad\", \"Pizza\")\n  ) +\n  coord_equal() +\n  theme_minimal()+\n  theme_enhance_waffle()",
    "crumbs": [
      "Waffle"
    ]
  },
  {
    "objectID": "pages/waffle.html#mn-economic-industry-sectors-2017",
    "href": "pages/waffle.html#mn-economic-industry-sectors-2017",
    "title": "Waffle",
    "section": "MN Economic Industry Sectors 2017",
    "text": "MN Economic Industry Sectors 2017\n\nprop_eco &lt;- mn_eco_industry |&gt;\n  group_by(sector_label) |&gt;\n  summarize(total = sum(estab)) |&gt;\n  mutate(prop = (total * 100 / sum(total)) + 0.35)\n\nparts &lt;- tibble(names = prop_eco$sector_label, \n                vals = prop_eco$prop)\n\nwaffle(parts, \n       rows = 10,\n       size = 2,\n       pad = 4,\n       legend_pos = \"bottom\",\n       title = \"Minnesota Economic Industry Proportion by Sector\")\n\n\n\n\n\n\n\n\n\nprop_eco |&gt;\n  ggplot(\n    aes(fill = sector_label, values = prop)\n  ) +\ngeom_waffle(\n    n_rows = 10,\n    size = 1.5, \n    colour = \"black\",\n    flip = FALSE,\n  ) +\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#0A9396\", \"#94D2BD\", \"#E9D8A6\", \"#EE9B00\", \"#CA7902\", \"#9B2226\")\n  ) +\n  coord_equal() +\n  labs(\n    title = \"Minnesota Economic Industry\",\n    subtitle = \"Proportion of Establishments by Sector\",\n    caption = \"Source: U.S. Census Bureau 2017\"\n  ) +\n  dark_theme_minimal(base_size = 14)+\n  theme_enhance_waffle()",
    "crumbs": [
      "Waffle"
    ]
  },
  {
    "objectID": "pages/waffle.html#mn-economic-industry-retail-subsectors-2017",
    "href": "pages/waffle.html#mn-economic-industry-retail-subsectors-2017",
    "title": "Waffle",
    "section": "MN Economic Industry Retail Subsectors 2017",
    "text": "MN Economic Industry Retail Subsectors 2017\n\nretail &lt;- mn_eco_industry |&gt;\n  filter(sector_label == \"Retail Trade\") |&gt;\n  mutate(simplified_label = tolower(naics2017_label)) |&gt;\n  mutate(simplified_label = ifelse(grepl(\"stores\", simplified_label), \"Stores\", simplified_label),\n         simplified_label = ifelse(grepl(\"dealers\", simplified_label), \"Dealers\", simplified_label),\n         simplified_label = ifelse(grepl(\"markets\", simplified_label), \"Markets\", simplified_label),\n         simplified_label = ifelse(grepl(\"retailers\", simplified_label), \"Retailers\", simplified_label),\n         simplified_label = ifelse(grepl(\"stations\", simplified_label), \"Stations\", simplified_label),\n         simplified_label = ifelse(grepl(\"establishments\", simplified_label), \"Establishments\", simplified_label),\n         simplified_label = ifelse(grepl(\"houses\", simplified_label), \"Houses\", simplified_label),\n         simplified_label = ifelse(grepl(\"centers\", simplified_label), \"Centers\", simplified_label),\n         simplified_label = ifelse(grepl(\"operators\", simplified_label), \"Operators\", simplified_label),\n         simplified_label = ifelse(grepl(\"retail trade\", simplified_label), \"Retailers\", simplified_label),\n         simplified_label = ifelse(grepl(\"florists\", simplified_label), \"Stores\", simplified_label)) |&gt;\n  group_by(simplified_label) |&gt;\n  summarize(total = sum(napcsdol)) |&gt;\n  mutate(prop = (total * 100 / sum(total)) + 0.35)\n\n\nretail |&gt;\n  ggplot(\n    aes(fill = simplified_label, values = prop)\n  ) +\n  geom_waffle(\n    n_rows = 10,\n    size = 1.5, \n    colour = \"black\",\n    flip = FALSE,\n  ) +\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#0A9396\", \"#94D2BD\", \"#E9D8A6\", \"#EE9B00\", \"#CA7902\", \"#9B2226\")\n  ) +\n  coord_equal() +\n  labs(\n    title = \"Minnesota Retail Economic Sales\",\n    subtitle = \"Proportion of Sales by Sub Sector\",\n    caption = \"Source: U.S. Census Bureau 2017\"\n  ) +\n  dark_theme_minimal(base_size = 14)+\n  theme_enhance_waffle()\n\n\n\n\n\n\n\n\n\nretail_rm &lt;- retail |&gt;\n  filter(simplified_label != \"Markets\") |&gt;\n  filter(simplified_label != \"Operators\") |&gt;\n  filter(simplified_label != \"Establishments\")\n\nprop_eco_napcsdol &lt;- mn_eco_industry |&gt;\n  group_by(sector_label) |&gt;\n  summarize(total = sum(napcsdol)) |&gt;\n  mutate(prop = (total * 100 / sum(total)) + 0.35)\n\nfull_parts &lt;- tibble(names = prop_eco_napcsdol$sector_label, vals = prop_eco_napcsdol$prop)",
    "crumbs": [
      "Waffle"
    ]
  },
  {
    "objectID": "pages/waffle.html#iron-function",
    "href": "pages/waffle.html#iron-function",
    "title": "Waffle",
    "section": "Iron Function",
    "text": "Iron Function\nThe iron function is used to combine 2 or more waffle charts without using a facet.\n\nretail_parts &lt;- tibble(names = retail_rm$simplified_label, vals = retail_rm$prop)\n\nfull_parts &lt;- tibble(names = prop_eco_napcsdol$sector_label, vals = prop_eco_napcsdol$prop)\n\nw1 &lt;- waffle(full_parts,\n       rows = 10,\n       size = 2,\n       pad = 0,\n       colors = c(\"#0A9396\", \"#94D2BD\", \"#E9D8A6\", \"#EE9B00\", \"#CA7902\", \"#9B2226\"),\n       legend_pos = \"right\",\n       title = \"Minnesota Economic Sales Proportion by Sector\")\n\nw2 &lt;- waffle(retail_parts, \n         rows = 10,\n         size = 2,\n         pad = 0,\n         color = c(\"#461220\", \"#8c2f39\", \"#b23a48\", \"#fcb9b2\", \"#fed0bb\", \"#FFF1EB\"),\n         legend_pos = \"right\",\n         title = \"Retail Trade Economic Sales Proportion by Sub Sector\")\n\niron(w1, w2)\n\n\n\n\n\n\n\n\nThe waffle package is very useful in showing simple proportions. Other than that, it is not very useful for more complex data visualizations.",
    "crumbs": [
      "Waffle"
    ]
  },
  {
    "objectID": "pages/simulation.html",
    "href": "pages/simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "We will be using the Airbnb dataset (from data collected by St. Olaf Students in 2016) to simulate prices based on the district in Chicago. We will be using the Central, Northwest, North, and Southwest districts since these districts have the highest amount of listings. We will be filtering out any listings with less than 10 reviews in order to remove any new listings that may not have accurate prices. We will also be filtering out any prices over $300 in order to level the playing field with all districts (the central district has considerably higher price extremes than the other districts).\n\nairbnb &lt;- airbnb_raw |&gt;\n  filter(reviews &gt; 10) |&gt;\n  filter(price &lt; 300) |&gt;\n  filter(district %in% c(\"Southwest\", \"North\", \"Northwest\", \"Central\")) |&gt;\n  \n  mutate(district = case_when(\n    district == \"Southwest\" ~ \"SW\",\n    district == \"North\" ~ \"N\",\n    district == \"Northwest\" ~ \"NW\",\n    district == \"Central\" ~ \"C\"\n  )) |&gt;\n  mutate(district = as.factor(district)) |&gt;\n  select(price, bedrooms, accommodates, district)",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "pages/simulation.html#airbnb-data",
    "href": "pages/simulation.html#airbnb-data",
    "title": "Simulation",
    "section": "",
    "text": "We will be using the Airbnb dataset (from data collected by St. Olaf Students in 2016) to simulate prices based on the district in Chicago. We will be using the Central, Northwest, North, and Southwest districts since these districts have the highest amount of listings. We will be filtering out any listings with less than 10 reviews in order to remove any new listings that may not have accurate prices. We will also be filtering out any prices over $300 in order to level the playing field with all districts (the central district has considerably higher price extremes than the other districts).\n\nairbnb &lt;- airbnb_raw |&gt;\n  filter(reviews &gt; 10) |&gt;\n  filter(price &lt; 300) |&gt;\n  filter(district %in% c(\"Southwest\", \"North\", \"Northwest\", \"Central\")) |&gt;\n  \n  mutate(district = case_when(\n    district == \"Southwest\" ~ \"SW\",\n    district == \"North\" ~ \"N\",\n    district == \"Northwest\" ~ \"NW\",\n    district == \"Central\" ~ \"C\"\n  )) |&gt;\n  mutate(district = as.factor(district)) |&gt;\n  select(price, bedrooms, accommodates, district)",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "pages/simulation.html#exploratory-data-analysis",
    "href": "pages/simulation.html#exploratory-data-analysis",
    "title": "Simulation",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThe prices by bedrooms and accommodates show a positive correlation. However, there does not seem to be any relationship between district and either bedrooms or accommodates. We can see that prices are considerably higher in the Central district compared to the other districts. While the Southwest district has the lowest prices.\n\nggplot(airbnb, aes(x = price, y = bedrooms, color = district)) +\n  geom_point() +\n  labs(\n    title = \"Airbnb Prices by Bedrooms\",\n    x = \"Price\",\n    y = \"Bedrooms\",\n    color = \"District\"\n  ) +\n  dark_theme_gray(base_size = 12)\n\n\n\n\n\n\n\nggplot(airbnb, aes(x = price, y = accommodates, color = district)) +\n  geom_point() +\n  labs(\n    title = \"Airbnb Prices by Accommodates\",\n    x = \"Price\",\n    y = \"Accommodates\",\n    color = \"District\"\n  ) +\n  dark_theme_gray(base_size = 12)\n\n\n\n\n\n\n\nggplot(airbnb, aes(x = district, y = price, fill = district)) +\n  geom_boxplot() +\n  labs(\n    title = \"Airbnb Prices by District\",\n    x = \"District\",\n    y = \"Price\",\n    fill = \"District\"\n  ) +\n  dark_theme_gray(base_size = 12)",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "pages/simulation.html#generating-prices",
    "href": "pages/simulation.html#generating-prices",
    "title": "Simulation",
    "section": "Generating Prices",
    "text": "Generating Prices\nThis function generates prices based on the true mean and standard deviation. It also takes in the districts and the number of listings in each district. This function will be used later to simulate data for the Central, Northwest, North, and Southwest districts.\n\ngen_prices &lt;- function(true_mean, true_sd, districts, num_in_district, id = 1) {\n  table &lt;- tibble(\n    district = factor(),\n    price = numeric(),\n    id = character()\n  )\n\n  for (i in 1:length(districts)) {\n    sample &lt;- rnorm(num_in_district$n[i], mean = true_mean, sd = true_sd)\n    table &lt;- bind_rows(table, tibble(district = rep(districts[i], num_in_district$n[i]), price = sample, id = rep(id, num_in_district$n[i])))\n  }\n\n  return(table)\n}",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "pages/simulation.html#defining-parameters",
    "href": "pages/simulation.html#defining-parameters",
    "title": "Simulation",
    "section": "Defining Parameters",
    "text": "Defining Parameters\n\nprice_mean &lt;- mean(airbnb$price)\nprice_sd &lt;- sd(airbnb$price)\n\nnum_in_district &lt;- airbnb |&gt;\n  group_by(district) |&gt;\n  summarize(n = n())\n\nmean_num_in_district &lt;- mean(num_in_district$n)",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "pages/simulation.html#simulating-data",
    "href": "pages/simulation.html#simulating-data",
    "title": "Simulation",
    "section": "Simulating Data",
    "text": "Simulating Data\n\nn_sims &lt;- 8\nparams &lt;- tibble(\n  sd = c(rep(price_sd, n_sims)),\n  id = c(paste(\"Sim\", 1:n_sims))\n)\n\nsimulation &lt;- params %&gt;%\n  pmap_dfr(~ gen_prices(true_mean = price_mean, true_sd = ..1, districts = c(\"SW\", \"N\", \"NW\", \"C\"), num_in_district = num_in_district, id = ..2))\n\nairbnb_format &lt;- airbnb |&gt;\n  mutate(id = \"Truth\") |&gt;\n  select(district, price, id)\n\nfinal_sim &lt;- rbind(simulation, airbnb_format)\n\nWe can determine from the simulated samples that there is a large difference from the total mean price of Airbnbs in the Chicago areas, specifically on Central and Southwest Chicago. For the North and Northwest regions, they appear to be consistent with the simulated box plots. The higher prices that we see with the central Airbnb’s could be related to its close proximity to downtown Chicago which would have a higher cost of living. Furthermore, Southwest Chicago might have fewer things to do and may not be as desirable for tourism. If we wanted to be sure of our findings, we could do a ANOVA statistical test.\n\nggplot(final_sim, aes(x = district, y = price, fill = district)) +\n  geom_boxplot() +\n  labs(\n    title = \"Simulated Prices by District\",\n    x = \"District\",\n    y = \"Price\",\n    fill = \"District\"\n  ) +\n  scale_y_continuous(limits = c(0, max(final_sim$price) + 100)) +\n  dark_theme_gray(base_size = 12) +\n  facet_wrap(~id)",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "pages/projects/lstm/index.html",
    "href": "pages/projects/lstm/index.html",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "",
    "text": "The goal of this model is to classify whether or not a IMDB review has positive or negative sentiment towards the movie it is reviewing. The Long Short Term Memory Model will be used for binary classification of positive and negative sentiment."
  },
  {
    "objectID": "pages/projects/lstm/index.html#dataset-description",
    "href": "pages/projects/lstm/index.html#dataset-description",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "Dataset Description",
    "text": "Dataset Description\nIMDB dataset with 50,000 movie reviews with sentiment intended for binary sentiment classification. The sentiment is either positive or negative. When put through the neural network, sentiment 0 is negative and 1 is positive. The review is raw text data from the IMDB website which needs to be cleaned. The data was uploaded to Kaggle in 2019, IMDB Dataset of 50K Movie Reviews. The data was most than likely scraped from the IMDB Website. Then the data was manually labeled as having positive or negative sentiment."
  },
  {
    "objectID": "pages/projects/lstm/index.html#what-is-lstm",
    "href": "pages/projects/lstm/index.html#what-is-lstm",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "What is LSTM?",
    "text": "What is LSTM?\nLong Short Term Memory is neural network that is ideal for processing and predicting data. LSTM is a recurrent neural network that uses context neurons for short term memory. These context neurons have their value updated when going through the current layer of the model. The neuron then resets at the end of the layer to zero. The purpose of the context neuron is to provide the recurrence of information back to previous neurons.\nLSTM uses gradient descent for the cost function of the neuron weights. Training a neural neural network uses backwards propagation between layers in the model.\nLSTM is also faster at learning and more successful than other models like Recurrent Neural Networks (RNNs). In order to use a neural network, the model needs lots of data to train the model. However, in the long run the model is be better than using a linear regression model where each variable is every word.\n\nLSTM Models use Three Gates (in this order):\n\nForget Gate\n\nThis gate determines what data is forgotten based on previous hidden neural network layers and the new data being inputted.\n\nInput Gate\n\nThis gate determines what data should be added to the long-term memory based on previous hidden layers and new data. This gate uses tanh activation [-1,1] for long term memory portion and sigmoid activation [0,1] for the filtering portion. Then these outputs are multiplied together and then added to the layer neuron.\n\nOutput Gate\n\nThis gate determines what data should be passed through to the next neural network layer. The gate uses tanh activation for outputting to a new layer."
  },
  {
    "objectID": "pages/projects/lstm/index.html#cleaning-data",
    "href": "pages/projects/lstm/index.html#cleaning-data",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "Cleaning Data",
    "text": "Cleaning Data\nThis text filtering will remove any links, html formatting, and emojis. We are also converting the sentiment to an integer so the model can use it properly. We also remove reviews that have more than 500 words and less than 20 words. We are removing any reviews that have more than 500 words since more words will slow down our model. We are also remove any reviews with less than 20 words since it is difficult to get sentiment based off of few words even for a human let alone a neural network.\n\nimdb_tbl_view &lt;- imdb_raw_tbl |&gt;\n  mutate(review = gsub(\"https?://.+\", \"\", review)) |&gt;\n  mutate(review = gsub(\"&lt;.*?&gt;\", \"\", review)) |&gt;\n  mutate(review = gsub(\"[^\\x01-\\x7F]\", \"\", review)) |&gt;\n  mutate(sentiment = as.factor(sentiment)) |&gt;\n  mutate(sentiment = as.integer(sentiment)) |&gt;\n  mutate(sentiment = sentiment - 1) |&gt;\n  filter(tokenizers::count_words(review) &lt; 500) |&gt;\n  filter(tokenizers::count_words(review) &gt; 20)\n\nimdb_tbl &lt;- imdb_tbl_view |&gt;\n  mutate(review = tolower(review)) |&gt;\n  mutate(review = gsub(\"[@#$%^&*()_+=\\\\|&lt;&gt;/~`&lt;&gt;]\", \"\", review)) |&gt;\n  mutate(review = gsub(\"[!;:,.?]\", \" \", review)) |&gt;\n  mutate(review = gsub(\"\\\\s+\", \" \", review))\n\nWe use the tokenizer to view the average number of words per review. The average number of words per review is a right skew distribution. The sentiment is close to evenly split among both positive and negative sentiment.\n\nimdb_tbl |&gt;\n  ggplot(aes(x = tokenizers::count_words(review)))+\n  geom_histogram(bins = 30)+\n  labs(title=\"Number of Words per Review\", \n       x=\"Words\", y=\"Count\")\n\n\n\n\n\n\n\nimdb_tbl |&gt;\n  reframe(sentiment) |&gt;\n  count(sentiment)\n\n# A tibble: 2 × 2\n  sentiment     n\n      &lt;dbl&gt; &lt;int&gt;\n1         0 23220\n2         1 22902"
  },
  {
    "objectID": "pages/projects/lstm/index.html#data-split",
    "href": "pages/projects/lstm/index.html#data-split",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "Data Split",
    "text": "Data Split\n\nset.seed(12345)\n\nimdb_split &lt;- initial_split(imdb_tbl)\n\nimdb_train &lt;- training(imdb_split)\nimdb_test &lt;- testing(imdb_split)"
  },
  {
    "objectID": "pages/projects/lstm/index.html#tokenize",
    "href": "pages/projects/lstm/index.html#tokenize",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "Tokenize",
    "text": "Tokenize\nWe must tokenize the reviews in order for the LSTM model to interpret each token individually instead of using the entire review as one input. The tokenizer also converts the entire dataset into integer id values which the neural network model can use. We are only considering reviews with a maximum review word length of 500 words and maximum number of words the model will use is 20,000. Ideally, we would increase the max_words and max_length values, but this would require more computational power and time.\n\nmax_words &lt;- 1e4\nmax_length &lt;- 500\n\nimdb_recipe &lt;- recipe(~review, imdb_train) |&gt;\n  step_tokenize(review) |&gt;\n  step_tokenfilter(review, max_tokens = max_words) |&gt;\n  step_sequence_onehot(review, sequence_length = max_length)\n\nimdb_prep &lt;- prep(imdb_recipe)\nimdb_train_token &lt;- bake(imdb_prep, new_data = NULL, composition = \"matrix\")"
  },
  {
    "objectID": "pages/projects/lstm/index.html#installing-keras-w-tensorflow",
    "href": "pages/projects/lstm/index.html#installing-keras-w-tensorflow",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "Installing Keras w/ Tensorflow",
    "text": "Installing Keras w/ Tensorflow\nFrom now on I have switched eval to false due to the compute intensive nature of Machine Learning. However, all of this code has been run on my personal machine.\n\nlibrary(reticulate)\nlibrary(keras)\n\ninstall_keras(Tensorflow = \"2.11.1\", restart_session = FALSE)"
  },
  {
    "objectID": "pages/projects/lstm/index.html#training-validation",
    "href": "pages/projects/lstm/index.html#training-validation",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "Training Validation",
    "text": "Training Validation\nSet up for validation in the training dataset. We are also tokenizing the entire validation using our previous recipe imdb prep. This is so we can test the results of our model without over fitting on the final testing dataset.\n\nset.seed(1234)\nimdb_value &lt;- validation_split(imdb_train, strata = sentiment)\n\nimdb_analysis &lt;- bake(imdb_prep, new_data = \n                        analysis(imdb_value$splits[[1]]), composition = \"matrix\")\n\n\nimdb_assess &lt;- bake(imdb_prep, new_data = assessment(imdb_value$splits[[1]]),\n                    composition = \"matrix\")\n\nsentiment_analysis &lt;- analysis(imdb_value$splits[[1]]) |&gt; pull(sentiment)\nsentiment_assess &lt;- assessment(imdb_value$splits[[1]]) |&gt; pull(sentiment)"
  },
  {
    "objectID": "pages/projects/lstm/index.html#classification-model",
    "href": "pages/projects/lstm/index.html#classification-model",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "Classification Model",
    "text": "Classification Model\nInitial layer embedding is used to convert tokens into a usable layer for the lstm model to process. Then the lstm uses long and short term memory logic to determine which tokens are important for classification. The final dense layer converts the output from the lstm layer into an interpretable output.\nThe LSTM layer uses a dropout of 0.4 to reduce over fitting of the training data. This is the same idea with recurrent dropout.\nThe sigmoid activation moves the range of the previous layer to a value between 0 and 1 and is used for classification.\n\nOptimizer: Adam\n\nThis optimizer algorithm is used for gradient descent and is an efficient choice for neural networks.\n\nLoss: Binary Crossentropy\n\nAlso called Log Loss, binary crossentropy evaluates how good the predicted probabilities are, using a log scale.\n\nMaximizing Metrics: Accuracy\n\nThe highest accuracy is the goal of the neural network.\n\n\n\nlstm_mod &lt;- keras_model_sequential() |&gt;\n  layer_embedding(input_dim = max_words + 1, output_dim = 32) |&gt;\n  layer_lstm(units = 32, dropout = 0.4, recurrent_dropout = 0.4) |&gt;\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nlstm_mod |&gt;\n  compile(\n    optimizer = \"adam\",\n    loss = \"binary_crossentropy\",\n    metrics = c(\"accuracy\")\n  )"
  },
  {
    "objectID": "pages/projects/lstm/index.html#fitting-model",
    "href": "pages/projects/lstm/index.html#fitting-model",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "Fitting Model",
    "text": "Fitting Model\nThis plot details the training and validation results after each epoch. An epoch is a phase of training where the model passes over the entire dataset. The batch size is the number of samples that will be used throughout the network. It will take that number of training samples and propagate it through the network and continue doing that until all samples have been propagated. This model takes a long time to train on my computer, since each review contains around 150 words and there are 34,600 reviews in the training dataset.\n\nval_history &lt;- lstm_mod |&gt;\n  fit(\n    imdb_analysis,\n    sentiment_analysis,\n    epochs = 5,\n    validation_data = list(imdb_assess, sentiment_assess),\n    batch_size = 500\n  )\n\nplot(val_history)\n\n\n\n\nEpoch\n\n\n\nval_res &lt;- evaluate(lstm_mod, imdb_assess, sentiment_assess)\nval_res\n\nThis is the Accuracy and Loss of the LSTM model on the training dataset. Loss represents the distance between the true sentiment and predicted sentiment by the model."
  },
  {
    "objectID": "pages/projects/lstm/index.html#predict-on-testing-data",
    "href": "pages/projects/lstm/index.html#predict-on-testing-data",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "Predict on Testing Data",
    "text": "Predict on Testing Data\nThe ROC Curve in the lower right corner of the graph which means that our model is extremely good at predicting the inverse of what the sentiment should be. I don’t know exactly what is going on here.\n\nimdb_test_assess &lt;- bake(imdb_prep, new_data = imdb_test, composition = \"matrix\")\n\nimdb_predict_raw &lt;- tibble(pred = lstm_mod |&gt; \n                              predict(imdb_test_assess)) |&gt;\n                              mutate(pred = as.numeric(pred))\n\nimdb_predict_raw |&gt;\n  mutate(sentiment = as.factor(imdb_test$sentiment)) |&gt;\n  roc_curve(sentiment, pred) |&gt;\n  autoplot()+\n  labs(title = \"ROC Curve\")\n\n\n\n\nROC\n\n\n\nimdb_predict_tensor &lt;- lstm_mod |&gt; \n                          predict(imdb_test_assess) |&gt; \n                          `&gt;`(0.5) |&gt; \n                          k_cast(\"int32\")\n\nimdb_predict &lt;- tibble(pred = as.array(imdb_predict_tensor))\n\nimdb_test_pred &lt;- imdb_test |&gt;\n  mutate(sentiment = as.factor(sentiment)) |&gt;\n  mutate(pred = as.factor(imdb_predict$pred))"
  },
  {
    "objectID": "pages/projects/lstm/index.html#confusion-matrix-and-accuracy",
    "href": "pages/projects/lstm/index.html#confusion-matrix-and-accuracy",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "Confusion Matrix and Accuracy",
    "text": "Confusion Matrix and Accuracy\nThe heat map shows the false positives and negatives that our model predicts on the testing dataset. The accuracy of this model is relatively high, however, with further tweaking or cross validation of many parameters, this could be much better. Ideally more epochs and increasing the max words would increase the accuracy of the model.\n\nimdb_test_pred |&gt;\n  conf_mat(sentiment, pred) |&gt;\n  autoplot(type = \"heatmap\")\n\nimdb_test_pred |&gt;\n  mutate(pred = as.numeric(pred) - 1) |&gt;\n  roc_curve(sentiment, pred) |&gt;\n  autoplot()\n\nimdb_test_pred |&gt;\n  accuracy(sentiment, pred)\n\nimdb_test_pred |&gt;\n  specificity(sentiment, pred)\nimdb_test_pred |&gt;\n  sensitivity(sentiment, pred)\n\n\n\n\nConfMat\n\n\n\n\n\nSensitivitySpecificity\n\n\n\n# False Positive, Sentiment Negative, Predict Positive\nerror_positive &lt;- imdb_test_pred |&gt;\n  filter(sentiment == 0) |&gt;\n  filter(pred == 1)\n\n# False Negative, Sentiment Positive, Predict Negative\nerror_negative &lt;- imdb_test_pred |&gt;\n  filter(sentiment == 1) |&gt;\n  filter(pred == 0)\n\nLet’s first take a look at one of the false positives:\n\nonce in a while a movie will sweep along that stuns you draws you in awes you and in the end leaves you with a renewed belief in the human race from the artistry form this is not it this is an action movie that lacks convincing action it stinks rent something else\n\nThe start of the review appears to be positive and the model predicts that. The ending of the review is what is important for the overall sentiment of the text. The model struggles with understanding the sentiment throughout the entire review. The important section of the review is the ending which contradicts the initial positive review.\nNow let’s look at one of the false negatives:\n\na lovely little b picture with all the usual joe lewis touches people ripping up pillows and auras of lurking fear also alas an ending that comes out of nowhere because apparently the auteur has lost interest in the movie or perhaps because as a b picture it has to fit into a slot\n\nThe start of the review appears to be positive which the model understands. However at the end of the review, the tone shifts to be negative sentiment. This is one issue with this dataset is that some of the reviews can be difficult to understand if a review is positive or not. In this case the individual who labeled this data had the opinion that it had positive sentiment towards the movie. In my opinion, I believe that the review has negative sentiment towards the movie due to the last line of the review.\nAnother main issue with this model is that it does not understand sarcasm. Sarcasm is difficult to train for; more complex models today have difficulty understanding it."
  },
  {
    "objectID": "pages/projects/lstm/index.html#testing-custom-reviews",
    "href": "pages/projects/lstm/index.html#testing-custom-reviews",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "Testing Custom Reviews",
    "text": "Testing Custom Reviews\n\ncustom_test &lt;- tibble(review = \"One of the best movies I have ever seen. \n                      The plot is terrific and the actors are flawless.\")\ncustom_test &lt;- custom_test |&gt;\n  add_row(review = \"How could Christopher Nolan make such a terrible film. \n          I thought he was one of the best directors of our time, but he failed horribly.\") |&gt;\n  add_row(review = \"The film started as a cinematic masterpiece, but ended as a flop. \n          It started out as stunning and beautiful film, but ended as a dumpster fire. \n          Genuinely disappointed.\") |&gt;\n  mutate(review = tolower(review)) |&gt;\n  mutate(review = gsub(\"[@#$%^&*()_+=\\\\|&lt;&gt;/~`&lt;&gt;]\", \"\", review)) |&gt;\n  mutate(review = gsub(\"[!;:,.?]\", \" \", review)) |&gt;\n  mutate(review = gsub(\"\\\\s+\", \" \", review))\n\ncustom_token &lt;- bake(imdb_prep, new_data = custom_test, composition = \"matrix\")\n\ncustom_predict &lt;- tibble(pred = lstm_mod |&gt; predict(custom_token)) |&gt;\n  mutate(review = custom_test$review) |&gt;\n  mutate(pred = as.numeric(pred))\n\nHere are some custom reviews:\n\n“One of the best movies I have ever seen. The plot is terrific and the actors are flawless.”\n\nThe first custom review is obviously a positive review and the model correctly identifies it as positive sentiment. (pred = 0.887)\n\n“How could Christopher Nolan make such a terrible film. I thought he was one of the best directors of our time, but he failed horribly.”\n\nThe second custom review is also pretty obviously a negative review and the model correctly identifies it as negative sentiment. (pred = 0.28)\n\n“The film started as a cinematic masterpiece, but ended as a flop. It started out as stunning and beautiful film, but ended as a dumpster fire. Genuinely disappointed.”\n\nThis final review is significantly more difficult and does not correctly identify the negative sentiment. The model struggles with contradicting sentiment. The seems to be a lot more positive sentiment, but the last two words illustrates the critique’s stance on that specific movie. (pred = 0.75)\nThe model does a great job with correctly identifying obvious sentiment reviews, but struggles with complicated wording and sarcasm."
  },
  {
    "objectID": "pages/projects/lstm/index.html#model-changes",
    "href": "pages/projects/lstm/index.html#model-changes",
    "title": "Long Short Term Memory on IMDB Reviews",
    "section": "Model Changes",
    "text": "Model Changes\n\nCross validation for parameters such as max words, epoch, batch size, or dropout\nAdding more layers to the LSTM model\nUsing different model like the Gated Recurrent Unit Model (GRU)\nNot removing capitals and some forms of punctuation (definitely messed with the model)"
  },
  {
    "objectID": "pages/maps/wisconsin_districs.html",
    "href": "pages/maps/wisconsin_districs.html",
    "title": "Wisconsin Districts",
    "section": "",
    "text": "district_elect &lt;- fec16::results_house |&gt;\n  select(-runoff_votes, -runoff_percent, -footnotes) |&gt;\n  mutate(district = parse_number(district_id)) |&gt;\n  group_by(state, district) |&gt;\n  summarize(\n    N = n(), \n    total_votes = sum(general_votes, na.rm = TRUE),\n    d_votes = sum(ifelse(party == \"DEM\", general_votes, 0), na.rm = TRUE),\n    r_votes = sum(ifelse(party == \"REP\", general_votes, 0), na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    other_votes = total_votes - d_votes - r_votes,\n    r_prop = r_votes / total_votes,  \n    winner = ifelse(r_votes &gt; d_votes, \"Republican\", \"Democrat\")\n  )\nwi_results &lt;- district_elect |&gt;\n  filter(state == \"WI\")\n\nwi_d3 &lt;- fec16::results_house |&gt;\n  select(-runoff_votes, -runoff_percent, -footnotes) |&gt;\n  filter(state == \"WI\") |&gt;\n  filter(district_id == \"03\")\n\nwi_d3\n\n# A tibble: 2 × 10\n  state district_id cand_id   incumbent party primary_votes primary_percent\n  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;lgl&gt;     &lt;chr&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1 WI    03          H6WI03099 TRUE      DEM           33320           0.812\n2 WI    03          H6WI03164 FALSE     DEM            7689           0.187\n# ℹ 3 more variables: general_votes &lt;dbl&gt;, general_percent &lt;dbl&gt;, won &lt;lgl&gt;\n\n\nWisconsin District 3 had 2 democrats running for the house seat.\n\nsrc &lt;- \"http://cdmaps.polisci.ucla.edu/shp/districts113.zip\"\nlcl_zip &lt;- fs::path(tempdir(), \"districts113.zip\")\ndownload.file(src, destfile = lcl_zip)\nlcl_districts &lt;- fs::path(tempdir(), \"districts113\")\nunzip(lcl_zip, exdir = lcl_districts)\ndsn_districts &lt;- fs::path(lcl_districts, \"districtShapes\")\n\ndistricts &lt;- st_read(dsn_districts, layer = \"districts113\") |&gt;\n  mutate(DISTRICT = parse_number(as.character(DISTRICT)))\n\nReading layer `districts113' from data source \n  `/tmp/RtmpuYShoT/districts113/districtShapes' using driver `ESRI Shapefile'\nSimple feature collection with 436 features and 15 fields (with 1 geometry empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1473 ymin: 18.91383 xmax: 179.7785 ymax: 71.35256\nGeodetic CRS:  NAD83\n\nwi_shp &lt;- districts |&gt;\n  filter(STATENAME == \"Wisconsin\")\n\n\nwi_merged &lt;- wi_shp |&gt;\n  st_transform(4326) |&gt;\n  inner_join(wi_results, by = c(\"DISTRICT\" = \"district\"))\n\nwi &lt;- ggplot(data = wi_merged, aes(fill = winner)) +\n  annotation_map_tile(zoom = 6, type = \"osm\", progress = \"none\") + \n  geom_sf(alpha = 0.5) +\n  scale_fill_manual(\"Winner\", values = c(\"blue\", \"red\")) + \n  geom_sf_label(aes(label = DISTRICT)) + \n  ggtitle(\"Wisconsin House Winners by District\") +\n  labs(x = element_blank(), y = element_blank())+\n  dark_theme_gray(base_size = 12) + \n  theme(\n        axis.ticks = element_blank(), \n        axis.text = element_blank(), \n        plot.margin = unit(c(0.2, 0.7, 0, 1.5), \"cm\"))\n\nwi\n\n\n\n\n\n\n\nwi_results |&gt;                  \n  select(-state)\n\n# A tibble: 8 × 8\n  district     N total_votes d_votes r_votes other_votes r_prop winner    \n     &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1        1     7      353990  107003  230072       16915  0.650 Republican\n2        2     2      397581  273537  124044           0  0.312 Democrat  \n3        3     2      257401  257401       0           0  0     Democrat  \n4        4     4      285858  220181       0       65677  0     Democrat  \n5        5     3      390507  114477  260706       15324  0.668 Republican\n6        6     4      356935  133072  204147       19716  0.572 Republican\n7        7     4      362061  138643  223418           0  0.617 Republican\n8        8     4      363574  135682  227892           0  0.627 Republican\n\n\nThe map shows the winners of the 2016 House elections in Wisconsin. The district shapes are very irregular. District 4 is extremely small when compared to district 7. There are 2 districts where there isn’t even a republican running. The races where republicans have won the district, they have won by a small margin. However, the margin is not as small as the North Carolina districts that we looked at in class.",
    "crumbs": [
      "Maps: Wisconsin Districts"
    ]
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About",
    "section": "",
    "text": "My name is Jaden Chant. I am a Computer Science and Math Student at St. Olaf College. I am a member of the St. Olaf swim team. I am interested in front-end development and data science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jaden Chant",
    "section": "",
    "text": "Current CS and Math Student @ St. Olaf College"
  },
  {
    "objectID": "index.html#jaden-chant",
    "href": "index.html#jaden-chant",
    "title": "Jaden Chant",
    "section": "",
    "text": "Current CS and Math Student @ St. Olaf College"
  },
  {
    "objectID": "pages/maps/us_states.html",
    "href": "pages/maps/us_states.html",
    "title": "US States",
    "section": "",
    "text": "poverty_data &lt;- read_csv(\"https://jadenchant.github.io/data/poverty_estimates.csv\")\n\npoverty_data &lt;- poverty_data |&gt;\n  filter(FIPS_code %% 1000 == 0 & FIPS_code != 0) |&gt;\n  mutate(state = str_squish(str_to_lower(as.character(area_name)))) |&gt;\n  mutate(state = gsub(\" \", \"\", state)) |&gt;\n  select(state, everything(), -state_code, -area_name)\n\nThe data is from the USDA’s Economic Research Service and is from 2021. Cleaning the data yields a dataset that can be joined by the us_states dataset. The us_states dataset is a dataset that contains the longitudes and latitudes of the states in the United States.\n\npoverty_data |&gt;\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt;\n  ggplot(mapping = aes(\n    x = long, y = lat,\n    group = group\n  )) +\n  geom_polygon(aes(fill = PCTPOVALL_2021), color = \"#fefefe\") +\n  labs(fill = \"Percent Poverty \\nAll Ages\") +\n  labs(x = element_blank(), y = element_blank()) +\n  ggtitle(\"Percent Poverty by State\") +\n  coord_map() +\n  theme_void() +\n  scale_fill_viridis(option = \"C\") +\n  dark_theme_gray(base_size = 12) +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    plot.margin = unit(c(2, 0, 2, 0), \"cm\")\n  )\n\n\n\n\n\n\n\npoverty_data |&gt;\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt;\n  ggplot(mapping = aes(\n    x = long, y = lat,\n    group = group\n  )) +\n  geom_polygon(aes(fill = PCTPOV017_2021), color = \"#fefefe\") +\n  labs(fill = \"Percent Poverty \\nAges 0-17\") +\n  labs(x = element_blank(), y = element_blank()) +\n  ggtitle(\"Percent Poverty Ages 0-17 by State\") +\n  coord_map() +\n  theme_void() +\n  scale_fill_viridis(option = \"C\") +\n  dark_theme_gray(base_size = 12) +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    plot.margin = unit(c(2, 0, 2, 0), \"cm\")\n  )\n\n\n\n\n\n\n\npoverty_data |&gt;\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt;\n  ggplot(mapping = aes(\n    x = long, y = lat,\n    group = group\n  )) +\n  geom_polygon(aes(fill = MEDHHINC_2021), color = \"#fefefe\") +\n  labs(fill = \"Estimated Median \\nHousehold Income\") +\n  labs(x = element_blank(), y = element_blank()) +\n  ggtitle(\"Estimated Median Household Income by State\") +\n  coord_map() +\n  theme_void() +\n  scale_fill_viridis(option = \"D\", direction = -1) +\n  dark_theme_gray(base_size = 12) +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    plot.margin = unit(c(2, 0, 2, 0), \"cm\")\n  )\n\n\n\n\n\n\n\n\nThese maps show the percent poverty based on age by state level and show the estimated household income by state level. We can see the relationship between poverty and median household income. We also can see the relationship between region and poverty.",
    "crumbs": [
      "Maps: US States"
    ]
  },
  {
    "objectID": "pages/projects.html",
    "href": "pages/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nNFL Fantasy Point Prediction\n\n\n\nStats2\n\n\n\nNFL WR & TE Fantasy Football Point Prediction\n\n\n\nBen Gusdal and Jaden Chant\n\n\n\n\n\n\n\n\n\n\n\n\nLong Short Term Memory on IMDB Reviews\n\n\n\nADM\n\n\n\nMachine Learning Long Short Term Memory on IMDB Reviews and Sentiment\n\n\n\nJaden Chant\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html",
    "href": "pages/projects/nfl_fantasy_pt/index.html",
    "title": "NFL Fantasy Point Prediction",
    "section": "",
    "text": "nfl &lt;- bind_rows(nfl_2018, nfl_2019, nfl_2020, nfl_2021, nfl_2022) %&gt;%\n  # Height conversion\n  mutate(HT = Ht....3) %&gt;%\n  mutate(FT = ifelse(str_detect(HT, \"May\"), 5, 6)) %&gt;%\n  mutate(IN = as.numeric(str_extract(HT, \"[0-9]+\"))) %&gt;%\n  mutate(HeightInch = IN + (12 * FT)) %&gt;%\n\n  # Changing col names\n  mutate(Weight= Wt.) %&gt;%\n  mutate(TD = TD...4) %&gt;%\n  mutate(GamesStarted  = GS...5) %&gt;%\n  mutate(PPR = PPR...6) %&gt;%\n  mutate(FanPt = FantPt...15) %&gt;%\n  mutate(CatchPercent =  `Ctch%`) %&gt;%\n  mutate(GamesPlayed = G) %&gt;%\n  mutate(NumTargets = Tgt) %&gt;%\n  mutate(NumReceptions = Rec) %&gt;%\n  mutate(Position = Pos) %&gt;%\n\n  # Create new col\n  mutate(FanPtNextSeason = NA) %&gt;%\n\n  # Convert WR/QB position into WR\n  mutate(Position = if_else(Position == \"WR/QB\", \"WR\", Position)) %&gt;%\n\n  # Choose second team if player played on 2 teams\n  mutate(Team = gsub(\"^.+,(.+)\", \"\\\\1\", Team)) %&gt;%\n\n  # Selecting columns used in analysis\n  select(Player, Season, Age, BMI, Weight, HeightInch, GamesPlayed, GamesStarted, NumTargets, NumReceptions, CatchPercent, Position, FanPt, FanPtNextSeason)\n\n\n# Getting Fantasy Points for the next season and putting it in a row with current season\nfor(i in 1:590) {\n  name &lt;- nfl$Player[i]\n  season &lt;- nfl$Season[i]\n\n  # Finding next season score\n  for(j in 1:590) {\n    if(nfl$Player[j] == name && nfl$Season[j] == season + 1) {\n      nfl$FanPtNextSeason[i] = nfl$FanPt[j]\n    }\n  }\n}\n\n# Dropping players that don't have 2 consecutive seasons\nnfl &lt;- nfl %&gt;%\n  drop_na()\n\n# 290/590 observations\n\nnfl &lt;- nfl %&gt;%\n  mutate(FanPtNextSeason_Center = FanPtNextSeason - median(FanPtNextSeason)) %&gt;%\n  mutate(Age_Center = Age - median(Age)) %&gt;%\n  mutate(GamesPlayed_Center = GamesPlayed - median(GamesPlayed)) %&gt;%\n  mutate(BMI_Center = BMI - median(BMI)) %&gt;%\n  mutate(NumTargets_Center = NumTargets - median(NumTargets)) %&gt;%\n  mutate(NumReceptions_Center = NumReceptions - median(NumReceptions)) %&gt;%\n  mutate(CatchPercent_Center = CatchPercent - median(CatchPercent)) %&gt;%\n  mutate(GamesStarted_Center = GamesStarted - median(GamesStarted)) %&gt;%\n  mutate(Weight_Center = Weight - median(Weight)) %&gt;%\n  mutate(HeightInch_Center = HeightInch - median(HeightInch))"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#cleaning-dataset",
    "href": "pages/projects/nfl_fantasy_pt/index.html#cleaning-dataset",
    "title": "NFL Fantasy Point Prediction",
    "section": "",
    "text": "nfl &lt;- bind_rows(nfl_2018, nfl_2019, nfl_2020, nfl_2021, nfl_2022) %&gt;%\n  # Height conversion\n  mutate(HT = Ht....3) %&gt;%\n  mutate(FT = ifelse(str_detect(HT, \"May\"), 5, 6)) %&gt;%\n  mutate(IN = as.numeric(str_extract(HT, \"[0-9]+\"))) %&gt;%\n  mutate(HeightInch = IN + (12 * FT)) %&gt;%\n\n  # Changing col names\n  mutate(Weight= Wt.) %&gt;%\n  mutate(TD = TD...4) %&gt;%\n  mutate(GamesStarted  = GS...5) %&gt;%\n  mutate(PPR = PPR...6) %&gt;%\n  mutate(FanPt = FantPt...15) %&gt;%\n  mutate(CatchPercent =  `Ctch%`) %&gt;%\n  mutate(GamesPlayed = G) %&gt;%\n  mutate(NumTargets = Tgt) %&gt;%\n  mutate(NumReceptions = Rec) %&gt;%\n  mutate(Position = Pos) %&gt;%\n\n  # Create new col\n  mutate(FanPtNextSeason = NA) %&gt;%\n\n  # Convert WR/QB position into WR\n  mutate(Position = if_else(Position == \"WR/QB\", \"WR\", Position)) %&gt;%\n\n  # Choose second team if player played on 2 teams\n  mutate(Team = gsub(\"^.+,(.+)\", \"\\\\1\", Team)) %&gt;%\n\n  # Selecting columns used in analysis\n  select(Player, Season, Age, BMI, Weight, HeightInch, GamesPlayed, GamesStarted, NumTargets, NumReceptions, CatchPercent, Position, FanPt, FanPtNextSeason)\n\n\n# Getting Fantasy Points for the next season and putting it in a row with current season\nfor(i in 1:590) {\n  name &lt;- nfl$Player[i]\n  season &lt;- nfl$Season[i]\n\n  # Finding next season score\n  for(j in 1:590) {\n    if(nfl$Player[j] == name && nfl$Season[j] == season + 1) {\n      nfl$FanPtNextSeason[i] = nfl$FanPt[j]\n    }\n  }\n}\n\n# Dropping players that don't have 2 consecutive seasons\nnfl &lt;- nfl %&gt;%\n  drop_na()\n\n# 290/590 observations\n\nnfl &lt;- nfl %&gt;%\n  mutate(FanPtNextSeason_Center = FanPtNextSeason - median(FanPtNextSeason)) %&gt;%\n  mutate(Age_Center = Age - median(Age)) %&gt;%\n  mutate(GamesPlayed_Center = GamesPlayed - median(GamesPlayed)) %&gt;%\n  mutate(BMI_Center = BMI - median(BMI)) %&gt;%\n  mutate(NumTargets_Center = NumTargets - median(NumTargets)) %&gt;%\n  mutate(NumReceptions_Center = NumReceptions - median(NumReceptions)) %&gt;%\n  mutate(CatchPercent_Center = CatchPercent - median(CatchPercent)) %&gt;%\n  mutate(GamesStarted_Center = GamesStarted - median(GamesStarted)) %&gt;%\n  mutate(Weight_Center = Weight - median(Weight)) %&gt;%\n  mutate(HeightInch_Center = HeightInch - median(HeightInch))"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#correlational-plot",
    "href": "pages/projects/nfl_fantasy_pt/index.html#correlational-plot",
    "title": "NFL Fantasy Point Prediction",
    "section": "Correlational plot",
    "text": "Correlational plot\n\ncorr &lt;- nfl %&gt;%\n  select(Age, BMI, Weight, HeightInch, GamesPlayed, GamesStarted, NumTargets, NumReceptions,  CatchPercent, FanPtNextSeason) %&gt;%\n  cor()\n\ncorrplot(corr, type=\"upper\")"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#variable-selection",
    "href": "pages/projects/nfl_fantasy_pt/index.html#variable-selection",
    "title": "NFL Fantasy Point Prediction",
    "section": "Variable Selection",
    "text": "Variable Selection"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#extra-models",
    "href": "pages/projects/nfl_fantasy_pt/index.html#extra-models",
    "title": "NFL Fantasy Point Prediction",
    "section": "Extra Models",
    "text": "Extra Models"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#centered-models",
    "href": "pages/projects/nfl_fantasy_pt/index.html#centered-models",
    "title": "NFL Fantasy Point Prediction",
    "section": "Centered Models",
    "text": "Centered Models"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#confidence-intervals",
    "href": "pages/projects/nfl_fantasy_pt/index.html#confidence-intervals",
    "title": "NFL Fantasy Point Prediction",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n# Game Statistics\nsignif(confint(FanStats_center_model), digits = 3)\n\n                      2.5 % 97.5 %\n(Intercept)          -8.540  4.010\nGamesPlayed_Center   -6.600 -0.963\nNumTargets_Center    -0.943  1.260\nNumReceptions_Center -0.419  2.750\nCatchPercent_Center  -2.150  0.573\nGamesStarted_Center  -4.080  0.825\n\nsummary(FanStats_center_model)\n\n\nCall:\nlm(formula = FanPtNextSeason_Center ~ GamesPlayed_Center + NumTargets_Center + \n    NumReceptions_Center + CatchPercent_Center + GamesStarted_Center, \n    data = nfl)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-102.003  -30.625   -3.243   28.436  154.445 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)           -2.2692     3.1881  -0.712  0.47718   \nGamesPlayed_Center    -3.7818     1.4318  -2.641  0.00872 **\nNumTargets_Center      0.1582     0.5595   0.283  0.77762   \nNumReceptions_Center   1.1675     0.8062   1.448  0.14866   \nCatchPercent_Center   -0.7900     0.6927  -1.140  0.25505   \nGamesStarted_Center   -1.6273     1.2458  -1.306  0.19252   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 41.91 on 284 degrees of freedom\nMultiple R-squared:  0.4118,    Adjusted R-squared:  0.4014 \nF-statistic: 39.76 on 5 and 284 DF,  p-value: &lt; 2.2e-16\n\n# Physical Player Statistics\nsignif(confint(body_center_model), digits = 3)\n\n                   2.5 % 97.5 %\n(Intercept)        14.00  59.70\nAge_Center         -3.27   0.67\nBMI_Center         10.90 109.00\nWeight_Center     -14.40  -2.23\nHeightInch_Center   8.38  79.40\n\nsummary(body_center_model)\n\n\nCall:\nlm(formula = FanPtNextSeason_Center ~ Age_Center + BMI_Center + \n    Weight_Center + HeightInch_Center, data = nfl)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-106.610  -37.936   -4.379   28.652  177.810 \nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)         36.829     11.622   3.169  0.00170 **\nAge_Center          -1.302      1.001  -1.300  0.19477   \nBMI_Center          59.738     24.826   2.406  0.01676 * \nWeight_Center       -8.294      3.081  -2.692  0.00753 **\nHeightInch_Center   43.866     18.031   2.433  0.01560 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 48.19 on 285 degrees of freedom\nMultiple R-squared:  0.2194,    Adjusted R-squared:  0.2085 \nF-statistic: 20.03 on 4 and 285 DF,  p-value: 1.5e-14\n\n# Full\nsignif(confint(full_center_model), digits = 3)\n\n                       2.5 % 97.5 %\n(Intercept)          -14.300 28.300\nAge_Center            -4.440 -0.980\nGamesPlayed_Center    -7.190 -1.550\nBMI_Center           -33.100 53.100\nNumTargets_Center     -1.160  0.984\nNumReceptions_Center  -0.284  2.790\nCatchPercent_Center   -1.710  0.972\nGamesStarted_Center   -2.090  2.930\nWeight_Center         -7.140  3.630\nHeightInch_Center    -22.000 40.600\n\nsummary(full_center_model)\n\n\nCall:\nlm(formula = FanPtNextSeason_Center ~ Age_Center + GamesPlayed_Center + \n    BMI_Center + NumTargets_Center + NumReceptions_Center + CatchPercent_Center + \n    GamesStarted_Center + Weight_Center + HeightInch_Center, \n    data = nfl)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-96.958 -28.198  -2.581  23.185 154.157 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)           6.99996   10.82847   0.646  0.51852   \nAge_Center           -2.70968    0.87859  -3.084  0.00225 **\nGamesPlayed_Center   -4.36783    1.43164  -3.051  0.00250 **\nBMI_Center            9.96998   21.88946   0.455  0.64912   \nNumTargets_Center    -0.08711    0.54432  -0.160  0.87296   \nNumReceptions_Center  1.25317    0.78081   1.605  0.10963   \nCatchPercent_Center  -0.36696    0.68003  -0.540  0.58989   \nGamesStarted_Center   0.41783    1.27597   0.327  0.74357   \nWeight_Center        -1.75252    2.73575  -0.641  0.52231   \nHeightInch_Center     9.29523   15.90165   0.585  0.55932   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 40.52 on 280 degrees of freedom\nMultiple R-squared:  0.4577,    Adjusted R-squared:  0.4403 \nF-statistic: 26.26 on 9 and 280 DF,  p-value: &lt; 2.2e-16\n\n# Stepwise\nsignif(confint(step_center_model), digits = 3)\n\n                      2.5 % 97.5 %\n(Intercept)          -4.220  7.020\nWeight_Center        -0.564 -0.179\nGamesPlayed_Center   -6.570 -1.600\nAge_Center           -4.330 -1.150\nNumReceptions_Center  0.929  1.320\n\nsummary(step_center_model)\n\n\nCall:\nlm(formula = FanPtNextSeason_Center ~ Weight_Center + GamesPlayed_Center + \n    Age_Center + NumReceptions_Center, data = nfl)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-97.837 -27.942  -2.885  23.270 153.838 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           1.40211    2.85589   0.491 0.623838    \nWeight_Center        -0.37131    0.09775  -3.799 0.000178 ***\nGamesPlayed_Center   -4.08451    1.26166  -3.237 0.001349 ** \nAge_Center           -2.74210    0.80671  -3.399 0.000772 ***\nNumReceptions_Center  1.12363    0.09873  11.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 40.37 on 285 degrees of freedom\nMultiple R-squared:  0.4522,    Adjusted R-squared:  0.4445 \nF-statistic: 58.81 on 4 and 285 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#multicollinearity",
    "href": "pages/projects/nfl_fantasy_pt/index.html#multicollinearity",
    "title": "NFL Fantasy Point Prediction",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nsignif(vif(full_center_model), digits = 3)\n\n          Age_Center   GamesPlayed_Center           BMI_Center \n                1.24                 1.41               343.00 \n   NumTargets_Center NumReceptions_Center  CatchPercent_Center \n               80.80                79.90                 4.92 \n GamesStarted_Center        Weight_Center    HeightInch_Center \n                2.04               974.00               306.00 \n\nsignif(vif(step_center_model), digits = 3)\n\n       Weight_Center   GamesPlayed_Center           Age_Center \n                1.25                 1.10                 1.05 \nNumReceptions_Center \n                1.29 \n\nsignif(vif(body_center_model), digits = 3)\n\n       Age_Center        BMI_Center     Weight_Center HeightInch_Center \n             1.14            312.00            874.00            278.00 \n\nsignif(vif(FanStats_center_model), digits = 3)\n\n  GamesPlayed_Center    NumTargets_Center NumReceptions_Center \n                1.32                79.90                79.70 \n CatchPercent_Center  GamesStarted_Center \n                4.78                 1.81"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#comparing-predicted-fantasy-points-and-actual",
    "href": "pages/projects/nfl_fantasy_pt/index.html#comparing-predicted-fantasy-points-and-actual",
    "title": "NFL Fantasy Point Prediction",
    "section": "Comparing Predicted Fantasy Points and Actual",
    "text": "Comparing Predicted Fantasy Points and Actual\n\n# Final with point labels\nggplot(step_model_data, aes(x = fitted, y = FanPtNextSeason))+\n  geom_point()+\n  geom_abline(intercept = 0, slope = 1, color=\"red\")+\n  labs(title=\"Predicted vs. Actual Fantasy Points Next Season\", x=\"Predicted Fantasy Points Next Season\", y=\"Fantasy Points Next Season\")+\n  geom_text_repel(aes(fitted, FanPtNextSeason, label = paste(Player, Season + 1)))"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#outliers-cooks-distance",
    "href": "pages/projects/nfl_fantasy_pt/index.html#outliers-cooks-distance",
    "title": "NFL Fantasy Point Prediction",
    "section": "Outliers (Cook’s Distance)",
    "text": "Outliers (Cook’s Distance)\n\ndiag_step_model &lt;- ls.diag(step_center_model)\n\nstep_model_data &lt;- nfl %&gt;%\n  mutate(cooks = diag_step_model$cooks) %&gt;%\n  mutate(PredictedFanPtNextSeason = fitted(step_center_model))\n\nstep_model_data %&gt;%\n  filter(cooks &gt; 4/(290 - 5 - 1)) %&gt;%\n  mutate(NumTarg = NumTargets_Center, NumRec = NumReceptions_Center, GamesStarted = GamesStarted_Center, Age = Age_Center) %&gt;%\n  select(Player, Season, FanPtNextSeason, PredictedFanPtNextSeason, NumTarg, NumRec, GamesStarted, Age, Position, cooks) %&gt;%\n  arrange(desc(cooks)) %&gt;%\n  # Creating a table\n  kbl() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlayer\nSeason\nFanPtNextSeason\nPredictedFanPtNextSeason\nNumTarg\nNumRec\nGamesStarted\nAge\nPosition\ncooks\n\n\n\n\nAdam Thielen\n2019\n180.0\n90.762077\n-45\n-29.5\n-4\n3\nWR\n0.0355837\n\n\nTravis Kelce\n2021\n206.3\n107.271500\n41\n32.5\n2\n6\nTE\n0.0346981\n\n\nJuJu Smith-Schuster\n2018\n71.2\n169.037223\n73\n51.5\n-1\n-4\nWR\n0.0296657\n\n\nTyreek Hill\n2019\n241.9\n126.508221\n-4\n-1.5\n-2\n-1\nWR\n0.0253841\n\n\nCooper Kupp\n2020\n294.5\n140.661510\n31\n32.5\n-2\n1\nWR\n0.0252154\n\n\nTravis Kelce\n2019\n207.8\n118.373828\n43\n37.5\n2\n4\nTE\n0.0240142\n\n\nCooper Kupp\n2018\n176.5\n116.308718\n-38\n-19.5\n-6\n-1\nWR\n0.0232316\n\n\nDavante Adams\n2019\n243.4\n140.203229\n34\n23.5\n-2\n1\nWR\n0.0225230\n\n\nCooper Kupp\n2021\n126.4\n189.302554\n98\n85.5\n3\n2\nWR\n0.0210253\n\n\nDarren Fells\n2018\n76.1\n8.831637\n-81\n-48.5\n-3\n6\nTE\n0.0200830\n\n\nDavante Adams\n2021\n235.5\n163.326030\n76\n63.5\n2\n3\nWR\n0.0174965\n\n\nJalen Reagor\n2020\n45.1\n108.995378\n-39\n-28.5\n-3\n-5\nWR\n0.0173854\n\n\nJulio Jones\n2020\n49.4\n103.675779\n-25\n-8.5\n-5\n5\nWR\n0.0163152\n\n\nDarnell Mooney\n2021\n61.5\n141.354755\n47\n21.5\n0\n-2\nWR\n0.0153599\n\n\nAllen Robinson\n2020\n49.0\n143.357545\n58\n42.5\n2\n1\nWR\n0.0144785"
  },
  {
    "objectID": "pages/str.html",
    "href": "pages/str.html",
    "title": "RegEx",
    "section": "",
    "text": "We as humans can easily determine if someone is saying something positive or negative, but computers and algorithms have struggled with this for decades. The context and sarcasm is not easily discernible to a computer. I will be attempting to predict the sentiment from movie reviews on IMDB.\nThe dataset IMDB Reviews contains reviews from the IMDB website and the sentiment towards that specific movie review. There is an exactly 50/50 split between positive sentiment and negative sentiment. I previously used this dataset for a Machine learning project using Long Short Term Memory. Using ML I was able to get a 88.8% accuracy in correctly identifying the sentiment. One major flaw with this dataset is the accuracy of the data. There are many examples where it says that the review has negative sentiment, but is actually a positive review. A better dataset would have their actual star rating system which the user would input and would not be up for interpretation.\nCleaning the imdb dataset includes removing links, html tags, and special characters. We will also filter out reviews with more than 500 words and less than 20 words. It is only filtering out around 4000 reviews which is not a significant portion of the whole dataset.\n\nimdb_clean &lt;- imdb |&gt;\n  mutate(id = row_number()) |&gt;\n  mutate(review = str_replace_all(review, \"https?://.+\", \"\")) |&gt;\n  mutate(review = str_replace_all(review, \"&lt;.*?&gt;\", \"\")) |&gt;\n  mutate(review = str_replace_all(review, \"[^\\x01-\\x7F]\", \"\")) |&gt;\n  mutate(true_sentiment = as.factor(sentiment)) |&gt;\n  select(-sentiment) |&gt;\n  filter(tokenizers::count_words(review) &lt; 500) |&gt;\n  filter(tokenizers::count_words(review) &gt; 20)\n\nFirst we will take a smaller sample from the entire imdb_clean which will be easier to determine if the code will function correctly.\n\nimdb_small &lt;- slice_sample(imdb_clean, n = 250) |&gt;\n  mutate(id = row_number()) |&gt;\n  select(id, review, true_sentiment)\n\nimdb_small |&gt;\n  group_by(true_sentiment) |&gt;\n  summarize(n = n())\n\n# A tibble: 2 × 2\n  true_sentiment     n\n  &lt;fct&gt;          &lt;int&gt;\n1 negative         117\n2 positive         133\n\n\nThe Bing Sentiments include a tibble of 6,786 words with sentiment associated with them. These words will be joined with the reviews and\n\nbing_sentiment &lt;- get_sentiments(\"bing\")\n\ntokenized &lt;- imdb_small |&gt;\n  select(-true_sentiment) |&gt;\n  mutate(id = row_number(), \n         review = as.character(review)) |&gt;\n  unnest_tokens(word, review)\n\nsentiment_count &lt;- tokenized |&gt;\n  inner_join(bing_sentiment, by = \"word\") |&gt;\n  group_by(id, sentiment) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = sentiment, values_from = count, values_fill = 0)\n\nresult &lt;- left_join(imdb_small, sentiment_count, by = \"id\") |&gt;\n  mutate(pred_sentiment = as.factor(ifelse(positive &gt; negative, \"positive\", \"negative\"))) |&gt;\n  mutate(pred_val = positive / (positive + negative)) |&gt;\n  select(review, true_sentiment, pred_sentiment, pred_val, positive, negative)\n\nThe first histogram shows the true positive and true negative sentiments and their distribution of positive sentiment words. The second histogram shows the true positive and true negative sentiments and their distribution of negative sentiment words. All of the distributions look relatively the same.\n\nggplot(result, aes(x = positive, fill = true_sentiment))+\n  geom_histogram(binwidth = 3)+\n  facet_wrap(~true_sentiment)+\n  scale_fill_manual(values = c(\"positive\" = \"#0a821e\", \"negative\" = \"#8c160e\"))\n\n\n\n\n\n\n\nggplot(result, aes(x = negative, fill = true_sentiment))+\n  geom_histogram(binwidth = 3)+\n  facet_wrap(~true_sentiment)+\n  scale_fill_manual(values = c(\"positive\" = \"#0a821e\", \"negative\" = \"#8c160e\"))\n\n\n\n\n\n\n\n\nThe results will vary due to the sampling of imdb_clean rows. However, most of the time we can see that this model performs slightly better than just guessing what the sentiment is.\n\nresult |&gt;\n  conf_mat(true_sentiment, pred_sentiment) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\nresult |&gt;\n  accuracy(true_sentiment, pred_sentiment) |&gt;\n  pull(.estimate) |&gt;\n  cat(\"accuracy\")\n\n0.724 accuracy\n\nresult |&gt;\n  specificity(true_sentiment, pred_sentiment) |&gt;\n  pull(.estimate) |&gt;\n  cat(\"specificity\")\n\n0.7443609 specificity\n\nresult |&gt;\n  sensitivity(true_sentiment, pred_sentiment) |&gt;\n  pull(.estimate) |&gt;\n  cat(\"sensitivity\")\n\n0.7008547 sensitivity\n\n\nThe ROC curve shows us the relationship between specificity and sensitivity. The ideal situation is to have the highest sensitivity and specificity.\n\nresult |&gt;\n  roc_curve(true_sentiment, pred_val) |&gt;\n  autoplot()+\n  labs(title = \"ROC Curve\")\n\n\n\n\n\n\n\n\nThis code is the same as imdb_small, except for the filtering of na values at the end. I am assume this would be where there are no words in the review that match any of the bing_sentiment words.\n\ntokenized &lt;- imdb_clean |&gt;\n  select(-true_sentiment) |&gt;\n  mutate(id = row_number(), \n         review = as.character(review)) |&gt;\n  unnest_tokens(word, review)\n\nsentiment_count &lt;- tokenized |&gt;\n  inner_join(bing_sentiment, by = \"word\") |&gt;\n  group_by(id, sentiment) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = sentiment, values_from = count, values_fill = 0)\n\nresult_full &lt;- left_join(imdb_clean, sentiment_count, by = \"id\") |&gt;\n  mutate(pred_sentiment = as.factor(ifelse(positive &gt; negative, \"positive\", \"negative\"))) |&gt;\n  filter(!is.na(pred_sentiment)) |&gt;\n  mutate(pred_val = positive / (positive + negative)) |&gt;\n  select(review, true_sentiment, pred_sentiment, pred_val, positive, negative)\n\nThe histograms show the same outcomes as the imdb_small version, but the distributions look even closer to being the same.\n\nggplot(result_full, aes(x = positive, fill = true_sentiment))+\n  geom_histogram(binwidth = 5)+\n  facet_wrap(~true_sentiment)+\n  scale_fill_manual(values = c(\"positive\" = \"#0a821e\", \"negative\" = \"#8c160e\"))\n\n\n\n\n\n\n\nggplot(result_full, aes(x = negative, fill = true_sentiment))+\n  geom_histogram(binwidth = 5)+\n  facet_wrap(~true_sentiment)+\n  scale_fill_manual(values = c(\"positive\" = \"#0a821e\", \"negative\" = \"#8c160e\"))\n\n\n\n\n\n\n\n\nAs we can see from the confusion matrix and the accuracy, the model is not very good at predicting the sentiment of the reviews. The accuracy is 0.5 which is the same as a coin flip. It would be the same as guessing. This is to be expected since context matters much more than the individual words. This is why Machine Learning Models do better with text data that linear models or just counting words.\n\nresult_full |&gt;\n  conf_mat(true_sentiment, pred_sentiment) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\nresult_full |&gt;\n  accuracy(true_sentiment, pred_sentiment) |&gt;\n  pull(.estimate) |&gt;\n  cat(\"accuracy\")\n\n0.4998 accuracy\n\nresult_full |&gt;\n  specificity(true_sentiment, pred_sentiment) |&gt;\n  pull(.estimate) |&gt;\n  cat(\"specificity\")\n\n0.5188657 specificity\n\nresult_full |&gt;\n  sensitivity(true_sentiment, pred_sentiment) |&gt;\n  pull(.estimate) |&gt;\n  cat(\"sensitivity\")\n\n0.4809617 sensitivity\n\n\nThe ROC curve also confirms how terrible this model is for classification of sentiment in a large dataset setting. The sensitivity and specificity are both around 50%, which is why we are see a straight line for the ROC curve.\n\nresult_full |&gt;\n  roc_curve(true_sentiment, pred_val) |&gt;\n  autoplot()+\n  labs(title = \"ROC Curve\")\n\n\n\n\n\n\n\n\nOverall this method of sentiment binary classification is not a good method. The Machine Learning Model performs significantly better than purly counting the number of positive and negative sentimental words.",
    "crumbs": [
      "RegEx"
    ]
  }
]