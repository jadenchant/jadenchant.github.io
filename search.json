[
  {
    "objectID": "pages/str.html",
    "href": "pages/str.html",
    "title": "RegEx",
    "section": "",
    "text": "We as humans can easily determine if someone is saying something positive or negative, but computers and algorithms have struggled with this for decades. The context and sarcasm is not easily discernible to a computer. I will be attempting to predict the sentiment from movie reviews on IMDB.\nThe dataset IMDB Reviews contains reviews from the IMDB website and the sentiment towards that specific movie review. There is an exactly 50/50 split between positive sentiment and negative sentiment. I previously used this dataset for a Machine learning project using Long Short Term Memory. Using ML I was able to get a 88.8% accuracy in correctly identifying the sentiment. One major flaw with this dataset is the accuracy of the data. There are many examples where it says that the review has negative sentiment, but is actually a positive review. A better dataset would have their actual star rating system which the user would input and would not be up for interpretation.\n\nimdb_clean &lt;- imdb |&gt;\n  mutate(review = str_replace_all(review, \"https?://.+\", \"\")) |&gt;\n  mutate(review = str_replace_all(review, \"&lt;.*?&gt;\", \"\")) |&gt;\n  mutate(review = str_replace_all(review, \"[^\\x01-\\x7F]\", \"\")) |&gt;\n  mutate(sentiment = as.factor(sentiment)) |&gt;\n  mutate(sentiment = as.integer(sentiment)) |&gt;\n  mutate(sentiment = sentiment - 1) |&gt;\n  filter(tokenizers::count_words(review) &lt; 500) |&gt;\n  filter(tokenizers::count_words(review) &gt; 20)\n\n\nimdb |&gt;\n  filter(str_detect(review, \"positive\")) |&gt;\n  group_by(sentiment) |&gt;\n  summarize(n = n())\n\n# A tibble: 2 × 2\n  sentiment     n\n  &lt;chr&gt;     &lt;int&gt;\n1 negative    665\n2 positive    416\n\nimdb |&gt;\n  filter(str_detect(review, \"negative\")) |&gt;\n  group_by(sentiment) |&gt;\n  summarize(n = n())\n\n# A tibble: 2 × 2\n  sentiment     n\n  &lt;chr&gt;     &lt;int&gt;\n1 negative    338\n2 positive    363\n\n# sample()\n\nget_sentiments(\"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows",
    "crumbs": [
      "RegEx"
    ]
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html",
    "href": "pages/projects/nfl_fantasy_pt/index.html",
    "title": "NFL Fantasy Point Prediction",
    "section": "",
    "text": "# Age\nggplot(nfl, aes(Age))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~Age, data = nfl)\nage_model &lt;- lm(FanPt ~ Age, nfl)\nresid_panel(age_model)\n\n\n\n\n\n\n\n# Games Played\nggplot(nfl, aes(GamesPlayed))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~GamesPlayed, data = nfl)\nG_model &lt;- lm(FanPt ~GamesPlayed, nfl)\nresid_panel(G_model)\n\n\n\n\n\n\n\n# BMI\nggplot(nfl, aes(BMI))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~BMI, data = nfl)\nBMI_model &lt;- lm(FanPt ~ BMI, nfl)\nresid_panel(BMI_model)\n\n\n\n\n\n\n\n# Targets\nggplot(nfl, aes(NumTargets))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~NumTargets, data = nfl)\nTgt_model &lt;- lm(FanPt ~NumTargets, nfl)\nresid_panel(Tgt_model)\n\n\n\n\n\n\n\n# Receptions\nggplot(nfl, aes(NumReceptions))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~NumReceptions, data = nfl)\nRec_model &lt;- lm(FanPt ~ NumReceptions, nfl)\nresid_panel(Rec_model)\n\n\n\n\n\n\n\n# Catch Percent\nggplot(nfl, aes(CatchPercent))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~CatchPercent, data = nfl)\nCatchPercent_model &lt;- lm(FanPt ~ CatchPercent, nfl)\nresid_panel(CatchPercent_model)\n\n\n\n\n\n\n\n# Games Started\nggplot(nfl, aes(GamesStarted))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~GamesStarted, data = nfl)\nGS_model &lt;- lm(FanPt ~ GamesStarted, nfl)\nresid_panel(GS_model)\n\n\n\n\n\n\n\n# Player Weight\nggplot(nfl, aes(Weight))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~Weight, data = nfl)\nWt_model &lt;- lm(FanPt ~Weight, nfl)\nresid_panel(Wt_model)\n\n\n\n\n\n\n\n# Player Height\nggplot(nfl, aes(HeightInch))+\n  geom_histogram(bins = 7)\n\n\n\n\n\n\n\n# favstats(~HeightInch, data = nfl)\nHeightInch_model &lt;- lm(FanPt ~ HeightInch, nfl)\nresid_panel(HeightInch_model)\n\n\n\n\n\n\n\n# Player Position\nggplot(nfl, aes(Position))+\n  geom_bar()\n\n\n\n\n\n\n\ntable(nfl$Position)\n\n\n TE  WR \n 98 192 \n\nprop.table(table(nfl$Position))\n\n\n      TE       WR \n0.337931 0.662069 \n\n# Fantasy Points\nggplot(nfl, aes(FanPtNextSeason)) +\n  geom_histogram()\n\n\n\n\n\n\n\nggplot(nfl, aes(FanPtNextSeason)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# favstats(~FanPtNextSeason, data = nfl)\n\n\n# Age and FanPtNextSeason\nggplot(nfl, aes(Age, FanPtNextSeason))+\n  geom_smooth(method = \"lm\")+\n  geom_point()\n\n\n\n\n\n\n\n# BMI and FanPtNextSeason\nggplot(nfl, aes(BMI, FanPtNextSeason))+\n  geom_smooth(method = \"lm\")+\n  geom_point()\n\n\n\n\n\n\n\n# Height and FanPtNextSeason\nggplot(nfl, aes(HeightInch, FanPtNextSeason))+\n  geom_smooth(method = \"lm\")+\n  geom_point()\n\n\n\n\n\n\n\n# Position and Weight\nggplot(nfl, aes(Position, Weight, fill = Position))+\n  geom_violin()+\n  geom_boxplot(width=0.1, fill=\"white\") +\n  labs(title = \"Player Weight vs Player Position\")"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#eda",
    "href": "pages/projects/nfl_fantasy_pt/index.html#eda",
    "title": "NFL Fantasy Point Prediction",
    "section": "",
    "text": "# Age\nggplot(nfl, aes(Age))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~Age, data = nfl)\nage_model &lt;- lm(FanPt ~ Age, nfl)\nresid_panel(age_model)\n\n\n\n\n\n\n\n# Games Played\nggplot(nfl, aes(GamesPlayed))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~GamesPlayed, data = nfl)\nG_model &lt;- lm(FanPt ~GamesPlayed, nfl)\nresid_panel(G_model)\n\n\n\n\n\n\n\n# BMI\nggplot(nfl, aes(BMI))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~BMI, data = nfl)\nBMI_model &lt;- lm(FanPt ~ BMI, nfl)\nresid_panel(BMI_model)\n\n\n\n\n\n\n\n# Targets\nggplot(nfl, aes(NumTargets))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~NumTargets, data = nfl)\nTgt_model &lt;- lm(FanPt ~NumTargets, nfl)\nresid_panel(Tgt_model)\n\n\n\n\n\n\n\n# Receptions\nggplot(nfl, aes(NumReceptions))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~NumReceptions, data = nfl)\nRec_model &lt;- lm(FanPt ~ NumReceptions, nfl)\nresid_panel(Rec_model)\n\n\n\n\n\n\n\n# Catch Percent\nggplot(nfl, aes(CatchPercent))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~CatchPercent, data = nfl)\nCatchPercent_model &lt;- lm(FanPt ~ CatchPercent, nfl)\nresid_panel(CatchPercent_model)\n\n\n\n\n\n\n\n# Games Started\nggplot(nfl, aes(GamesStarted))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~GamesStarted, data = nfl)\nGS_model &lt;- lm(FanPt ~ GamesStarted, nfl)\nresid_panel(GS_model)\n\n\n\n\n\n\n\n# Player Weight\nggplot(nfl, aes(Weight))+\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n# favstats(~Weight, data = nfl)\nWt_model &lt;- lm(FanPt ~Weight, nfl)\nresid_panel(Wt_model)\n\n\n\n\n\n\n\n# Player Height\nggplot(nfl, aes(HeightInch))+\n  geom_histogram(bins = 7)\n\n\n\n\n\n\n\n# favstats(~HeightInch, data = nfl)\nHeightInch_model &lt;- lm(FanPt ~ HeightInch, nfl)\nresid_panel(HeightInch_model)\n\n\n\n\n\n\n\n# Player Position\nggplot(nfl, aes(Position))+\n  geom_bar()\n\n\n\n\n\n\n\ntable(nfl$Position)\n\n\n TE  WR \n 98 192 \n\nprop.table(table(nfl$Position))\n\n\n      TE       WR \n0.337931 0.662069 \n\n# Fantasy Points\nggplot(nfl, aes(FanPtNextSeason)) +\n  geom_histogram()\n\n\n\n\n\n\n\nggplot(nfl, aes(FanPtNextSeason)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# favstats(~FanPtNextSeason, data = nfl)\n\n\n# Age and FanPtNextSeason\nggplot(nfl, aes(Age, FanPtNextSeason))+\n  geom_smooth(method = \"lm\")+\n  geom_point()\n\n\n\n\n\n\n\n# BMI and FanPtNextSeason\nggplot(nfl, aes(BMI, FanPtNextSeason))+\n  geom_smooth(method = \"lm\")+\n  geom_point()\n\n\n\n\n\n\n\n# Height and FanPtNextSeason\nggplot(nfl, aes(HeightInch, FanPtNextSeason))+\n  geom_smooth(method = \"lm\")+\n  geom_point()\n\n\n\n\n\n\n\n# Position and Weight\nggplot(nfl, aes(Position, Weight, fill = Position))+\n  geom_violin()+\n  geom_boxplot(width=0.1, fill=\"white\") +\n  labs(title = \"Player Weight vs Player Position\")"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#correlational-plot",
    "href": "pages/projects/nfl_fantasy_pt/index.html#correlational-plot",
    "title": "NFL Fantasy Point Prediction",
    "section": "Correlational plot",
    "text": "Correlational plot\n\ncorr &lt;- nfl %&gt;%\n  select(Age, BMI, Weight, HeightInch, GamesPlayed, GamesStarted, NumTargets, NumReceptions,  CatchPercent, FanPtNextSeason) %&gt;%\n  cor()\n\ncorrplot(corr, type=\"upper\")"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#variable-selection",
    "href": "pages/projects/nfl_fantasy_pt/index.html#variable-selection",
    "title": "NFL Fantasy Point Prediction",
    "section": "Variable Selection",
    "text": "Variable Selection"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#extra-models",
    "href": "pages/projects/nfl_fantasy_pt/index.html#extra-models",
    "title": "NFL Fantasy Point Prediction",
    "section": "Extra Models",
    "text": "Extra Models"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#centered-models",
    "href": "pages/projects/nfl_fantasy_pt/index.html#centered-models",
    "title": "NFL Fantasy Point Prediction",
    "section": "Centered Models",
    "text": "Centered Models"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#confidence-intervals",
    "href": "pages/projects/nfl_fantasy_pt/index.html#confidence-intervals",
    "title": "NFL Fantasy Point Prediction",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n# Game Statistics\nsignif(confint(FanStats_center_model), digits = 3)\n\n                      2.5 % 97.5 %\n(Intercept)          -8.540  4.010\nGamesPlayed_Center   -6.600 -0.963\nNumTargets_Center    -0.943  1.260\nNumReceptions_Center -0.419  2.750\nCatchPercent_Center  -2.150  0.573\nGamesStarted_Center  -4.080  0.825\n\nsummary(FanStats_center_model)\n\n\nCall:\nlm(formula = FanPtNextSeason_Center ~ GamesPlayed_Center + NumTargets_Center + \n    NumReceptions_Center + CatchPercent_Center + GamesStarted_Center, \n    data = nfl)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-102.003  -30.625   -3.243   28.436  154.445 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)           -2.2692     3.1881  -0.712  0.47718   \nGamesPlayed_Center    -3.7818     1.4318  -2.641  0.00872 **\nNumTargets_Center      0.1582     0.5595   0.283  0.77762   \nNumReceptions_Center   1.1675     0.8062   1.448  0.14866   \nCatchPercent_Center   -0.7900     0.6927  -1.140  0.25505   \nGamesStarted_Center   -1.6273     1.2458  -1.306  0.19252   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 41.91 on 284 degrees of freedom\nMultiple R-squared:  0.4118,    Adjusted R-squared:  0.4014 \nF-statistic: 39.76 on 5 and 284 DF,  p-value: &lt; 2.2e-16\n\n# Physical Player Statistics\nsignif(confint(body_center_model), digits = 3)\n\n                   2.5 % 97.5 %\n(Intercept)        14.00  59.70\nAge_Center         -3.27   0.67\nBMI_Center         10.90 109.00\nWeight_Center     -14.40  -2.23\nHeightInch_Center   8.38  79.40\n\nsummary(body_center_model)\n\n\nCall:\nlm(formula = FanPtNextSeason_Center ~ Age_Center + BMI_Center + \n    Weight_Center + HeightInch_Center, data = nfl)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-106.610  -37.936   -4.379   28.652  177.810 \nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)         36.829     11.622   3.169  0.00170 **\nAge_Center          -1.302      1.001  -1.300  0.19477   \nBMI_Center          59.738     24.826   2.406  0.01676 * \nWeight_Center       -8.294      3.081  -2.692  0.00753 **\nHeightInch_Center   43.866     18.031   2.433  0.01560 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 48.19 on 285 degrees of freedom\nMultiple R-squared:  0.2194,    Adjusted R-squared:  0.2085 \nF-statistic: 20.03 on 4 and 285 DF,  p-value: 1.5e-14\n\n# Full\nsignif(confint(full_center_model), digits = 3)\n\n                       2.5 % 97.5 %\n(Intercept)          -14.300 28.300\nAge_Center            -4.440 -0.980\nGamesPlayed_Center    -7.190 -1.550\nBMI_Center           -33.100 53.100\nNumTargets_Center     -1.160  0.984\nNumReceptions_Center  -0.284  2.790\nCatchPercent_Center   -1.710  0.972\nGamesStarted_Center   -2.090  2.930\nWeight_Center         -7.140  3.630\nHeightInch_Center    -22.000 40.600\n\nsummary(full_center_model)\n\n\nCall:\nlm(formula = FanPtNextSeason_Center ~ Age_Center + GamesPlayed_Center + \n    BMI_Center + NumTargets_Center + NumReceptions_Center + CatchPercent_Center + \n    GamesStarted_Center + Weight_Center + HeightInch_Center, \n    data = nfl)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-96.958 -28.198  -2.581  23.185 154.157 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)           6.99996   10.82847   0.646  0.51852   \nAge_Center           -2.70968    0.87859  -3.084  0.00225 **\nGamesPlayed_Center   -4.36783    1.43164  -3.051  0.00250 **\nBMI_Center            9.96998   21.88946   0.455  0.64912   \nNumTargets_Center    -0.08711    0.54432  -0.160  0.87296   \nNumReceptions_Center  1.25317    0.78081   1.605  0.10963   \nCatchPercent_Center  -0.36696    0.68003  -0.540  0.58989   \nGamesStarted_Center   0.41783    1.27597   0.327  0.74357   \nWeight_Center        -1.75252    2.73575  -0.641  0.52231   \nHeightInch_Center     9.29523   15.90165   0.585  0.55932   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 40.52 on 280 degrees of freedom\nMultiple R-squared:  0.4577,    Adjusted R-squared:  0.4403 \nF-statistic: 26.26 on 9 and 280 DF,  p-value: &lt; 2.2e-16\n\n# Stepwise\nsignif(confint(step_center_model), digits = 3)\n\n                      2.5 % 97.5 %\n(Intercept)          -4.220  7.020\nWeight_Center        -0.564 -0.179\nGamesPlayed_Center   -6.570 -1.600\nAge_Center           -4.330 -1.150\nNumReceptions_Center  0.929  1.320\n\nsummary(step_center_model)\n\n\nCall:\nlm(formula = FanPtNextSeason_Center ~ Weight_Center + GamesPlayed_Center + \n    Age_Center + NumReceptions_Center, data = nfl)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-97.837 -27.942  -2.885  23.270 153.838 \nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           1.40211    2.85589   0.491 0.623838    \nWeight_Center        -0.37131    0.09775  -3.799 0.000178 ***\nGamesPlayed_Center   -4.08451    1.26166  -3.237 0.001349 ** \nAge_Center           -2.74210    0.80671  -3.399 0.000772 ***\nNumReceptions_Center  1.12363    0.09873  11.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 40.37 on 285 degrees of freedom\nMultiple R-squared:  0.4522,    Adjusted R-squared:  0.4445 \nF-statistic: 58.81 on 4 and 285 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#multicollinearity",
    "href": "pages/projects/nfl_fantasy_pt/index.html#multicollinearity",
    "title": "NFL Fantasy Point Prediction",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nsignif(vif(full_center_model), digits = 3)\n\n          Age_Center   GamesPlayed_Center           BMI_Center \n                1.24                 1.41               343.00 \n   NumTargets_Center NumReceptions_Center  CatchPercent_Center \n               80.80                79.90                 4.92 \n GamesStarted_Center        Weight_Center    HeightInch_Center \n                2.04               974.00               306.00 \n\nsignif(vif(step_center_model), digits = 3)\n\n       Weight_Center   GamesPlayed_Center           Age_Center \n                1.25                 1.10                 1.05 \nNumReceptions_Center \n                1.29 \n\nsignif(vif(body_center_model), digits = 3)\n\n       Age_Center        BMI_Center     Weight_Center HeightInch_Center \n             1.14            312.00            874.00            278.00 \n\nsignif(vif(FanStats_center_model), digits = 3)\n\n  GamesPlayed_Center    NumTargets_Center NumReceptions_Center \n                1.32                79.90                79.70 \n CatchPercent_Center  GamesStarted_Center \n                4.78                 1.81"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#comparing-predicted-fantasy-points-and-actual",
    "href": "pages/projects/nfl_fantasy_pt/index.html#comparing-predicted-fantasy-points-and-actual",
    "title": "NFL Fantasy Point Prediction",
    "section": "Comparing Predicted Fantasy Points and Actual",
    "text": "Comparing Predicted Fantasy Points and Actual\n\n# Final Model\nggplot(step_model_data, aes(x = fitted, y = FanPtNextSeason))+\n  geom_point()+\n  geom_abline(intercept = 0, slope = 1, color=\"red\")+\n  labs(title=\"Predicted vs. Actual Fantasy Points Next Season\", x=\"Predicted Fantasy Points Next Season\", y=\"Fantasy Points Next Season\")\n\n\n\n\n\n\n\n# Player Measurables\nggplot(body_center_data, aes(x = fitted, y = FanPtNextSeason))+\n  geom_point()+\n  geom_abline(intercept = 0, slope = 1, color=\"red\")+\n  labs(title=\"Predicted vs. Actual Fantasy Points Next Season (Player Measurables)\", x=\"Predicted Fantasy Points Next Season\", y=\"Fantasy Points Next Season\")\n\n\n\n\n\n\n\n# Onfield Stats\nggplot(FanStats_center_data, aes(x = fitted, y = FanPtNextSeason))+\n  geom_point()+\n  geom_abline(intercept = 0, slope = 1, color=\"red\")+\n  labs(title=\"Predicted vs. Actual Fantasy Points Next Season (Onfield Statistics)\", x=\"Predicted Fantasy Points Next Season\", y=\"Fantasy Points Next Season\")\n\n\n\n\n\n\n\n# Final with point labels\nggplot(step_model_data, aes(x = fitted, y = FanPtNextSeason))+\n  geom_point()+\n  geom_abline(intercept = 0, slope = 1, color=\"red\")+\n  labs(title=\"Predicted vs. Actual Fantasy Points Next Season\", x=\"Predicted Fantasy Points Next Season\", y=\"Fantasy Points Next Season\")+\n  geom_text_repel(aes(fitted, FanPtNextSeason, label = paste(Player, Season + 1)))"
  },
  {
    "objectID": "pages/projects/nfl_fantasy_pt/index.html#outliers-cooks-distance",
    "href": "pages/projects/nfl_fantasy_pt/index.html#outliers-cooks-distance",
    "title": "NFL Fantasy Point Prediction",
    "section": "Outliers (Cook’s Distance)",
    "text": "Outliers (Cook’s Distance)\n\ndiag_step_model &lt;- ls.diag(step_center_model)\n\nstep_model_data &lt;- nfl %&gt;%\n  mutate(cooks = diag_step_model$cooks) %&gt;%\n  mutate(PredictedFanPtNextSeason = fitted(step_center_model))\n\nstep_model_data %&gt;%\n  filter(cooks &gt; 4/(290 - 5 - 1)) %&gt;%\n  mutate(NumTarg = NumTargets_Center, NumRec = NumReceptions_Center, GamesStarted = GamesStarted_Center, Age = Age_Center) %&gt;%\n  select(Player, Season, FanPtNextSeason, PredictedFanPtNextSeason, NumTarg, NumRec, GamesStarted, Age, Position, cooks) %&gt;%\n  arrange(desc(cooks)) %&gt;%\n  # Creating a table\n  kbl() %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlayer\nSeason\nFanPtNextSeason\nPredictedFanPtNextSeason\nNumTarg\nNumRec\nGamesStarted\nAge\nPosition\ncooks\n\n\n\n\nAdam Thielen\n2019\n180.0\n90.762077\n-45\n-29.5\n-4\n3\nWR\n0.0355837\n\n\nTravis Kelce\n2021\n206.3\n107.271500\n41\n32.5\n2\n6\nTE\n0.0346981\n\n\nJuJu Smith-Schuster\n2018\n71.2\n169.037223\n73\n51.5\n-1\n-4\nWR\n0.0296657\n\n\nTyreek Hill\n2019\n241.9\n126.508221\n-4\n-1.5\n-2\n-1\nWR\n0.0253841\n\n\nCooper Kupp\n2020\n294.5\n140.661510\n31\n32.5\n-2\n1\nWR\n0.0252154\n\n\nTravis Kelce\n2019\n207.8\n118.373828\n43\n37.5\n2\n4\nTE\n0.0240142\n\n\nCooper Kupp\n2018\n176.5\n116.308718\n-38\n-19.5\n-6\n-1\nWR\n0.0232316\n\n\nDavante Adams\n2019\n243.4\n140.203229\n34\n23.5\n-2\n1\nWR\n0.0225230\n\n\nCooper Kupp\n2021\n126.4\n189.302554\n98\n85.5\n3\n2\nWR\n0.0210253\n\n\nDarren Fells\n2018\n76.1\n8.831637\n-81\n-48.5\n-3\n6\nTE\n0.0200830\n\n\nDavante Adams\n2021\n235.5\n163.326030\n76\n63.5\n2\n3\nWR\n0.0174965\n\n\nJalen Reagor\n2020\n45.1\n108.995378\n-39\n-28.5\n-3\n-5\nWR\n0.0173854\n\n\nJulio Jones\n2020\n49.4\n103.675779\n-25\n-8.5\n-5\n5\nWR\n0.0163152\n\n\nDarnell Mooney\n2021\n61.5\n141.354755\n47\n21.5\n0\n-2\nWR\n0.0153599\n\n\nAllen Robinson\n2020\n49.0\n143.357545\n58\n42.5\n2\n1\nWR\n0.0144785"
  },
  {
    "objectID": "pages/projects.html",
    "href": "pages/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nNFL Fantasy Point Prediction\n\n\n\nStats2\n\n\n\nNFL WR & TE Fantasy Football Point Prediction\n\n\n\nBen Gusdal and Jaden Chant\n\n\n\n\n\n\n\n\n\n\n\n\nML: Long Short Term Memory on IMDB Reviews\n\n\n\nADM\n\n\n\nMachine Learning Long Short Term Memory on IMDB Reviews and Sentiment\n\n\n\nJaden Chant\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/maps/us_states.html",
    "href": "pages/maps/us_states.html",
    "title": "US States",
    "section": "",
    "text": "poverty_data &lt;- read_csv(\"https://jadenchant.github.io/data/poverty_estimates.csv\")\n\npoverty_data &lt;- poverty_data |&gt;\n  filter(FIPS_code %% 1000 == 0 & FIPS_code != 0) |&gt;\n  mutate(state = str_squish(str_to_lower(as.character(area_name)))) |&gt;\n  mutate(state = gsub(\" \", \"\", state)) |&gt;\n  select(state, everything(), -state_code, -area_name)\n\nThe data is from the USDA’s Economic Research Service and is from 2021. Cleaning the data yields a dataset that can be joined by the us_states dataset. The us_states dataset is a dataset that contains the longitudes and latitudes of the states in the United States.\n\npoverty_data |&gt;\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt;\n  ggplot(mapping = aes(\n    x = long, y = lat,\n    group = group\n  )) +\n  geom_polygon(aes(fill = PCTPOVALL_2021), color = \"#fefefe\") +\n  labs(fill = \"Percent Poverty \\nAll Ages\") +\n  labs(x = element_blank(), y = element_blank()) +\n  ggtitle(\"Percent Poverty by State\") +\n  coord_map() +\n  theme_void() +\n  scale_fill_viridis(option = \"C\") +\n  dark_theme_gray(base_size = 12) +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    plot.margin = unit(c(2, 0, 2, 0), \"cm\")\n  )\n\n\n\n\n\n\n\npoverty_data |&gt;\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt;\n  ggplot(mapping = aes(\n    x = long, y = lat,\n    group = group\n  )) +\n  geom_polygon(aes(fill = PCTPOV017_2021), color = \"#fefefe\") +\n  labs(fill = \"Percent Poverty \\nAges 0-17\") +\n  labs(x = element_blank(), y = element_blank()) +\n  ggtitle(\"Percent Poverty Ages 0-17 by State\") +\n  coord_map() +\n  theme_void() +\n  scale_fill_viridis(option = \"C\") +\n  dark_theme_gray(base_size = 12) +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    plot.margin = unit(c(2, 0, 2, 0), \"cm\")\n  )\n\n\n\n\n\n\n\npoverty_data |&gt;\n  right_join(us_states, by = c(\"state\" = \"region\")) |&gt;\n  ggplot(mapping = aes(\n    x = long, y = lat,\n    group = group\n  )) +\n  geom_polygon(aes(fill = MEDHHINC_2021), color = \"#fefefe\") +\n  labs(fill = \"Estimated Median \\nHousehold Income\") +\n  labs(x = element_blank(), y = element_blank()) +\n  ggtitle(\"Estimated Median Household Income by State\") +\n  coord_map() +\n  theme_void() +\n  scale_fill_viridis(option = \"D\", direction = -1) +\n  dark_theme_gray(base_size = 12) +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    plot.margin = unit(c(2, 0, 2, 0), \"cm\")\n  )\n\n\n\n\n\n\n\n\nThese maps show the percent poverty based on age by state level and show the estimated household income by state level. We can see the relationship between poverty and median household income. We also can see the relationship between region and poverty.",
    "crumbs": [
      "Maps: US States"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jaden Chant",
    "section": "",
    "text": "Current CS and Math Student @ St. Olaf College"
  },
  {
    "objectID": "index.html#jaden-chant",
    "href": "index.html#jaden-chant",
    "title": "Jaden Chant",
    "section": "",
    "text": "Current CS and Math Student @ St. Olaf College"
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About",
    "section": "",
    "text": "My name is Jaden Chant. I am a Computer Science and Math Student at St. Olaf College. I am a member of the St. Olaf swim team. I am interested in front-end development and data science."
  },
  {
    "objectID": "pages/maps/wisconsin_districs.html",
    "href": "pages/maps/wisconsin_districs.html",
    "title": "Wisconsin Districts",
    "section": "",
    "text": "district_elect &lt;- fec16::results_house |&gt;\n  select(-runoff_votes, -runoff_percent, -footnotes) |&gt;\n  mutate(district = parse_number(district_id)) |&gt;\n  group_by(state, district) |&gt;\n  summarize(\n    N = n(), \n    total_votes = sum(general_votes, na.rm = TRUE),\n    d_votes = sum(ifelse(party == \"DEM\", general_votes, 0), na.rm = TRUE),\n    r_votes = sum(ifelse(party == \"REP\", general_votes, 0), na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    other_votes = total_votes - d_votes - r_votes,\n    r_prop = r_votes / total_votes,  \n    winner = ifelse(r_votes &gt; d_votes, \"Republican\", \"Democrat\")\n  )\nwi_results &lt;- district_elect |&gt;\n  filter(state == \"WI\")\n\nwi_d3 &lt;- fec16::results_house |&gt;\n  select(-runoff_votes, -runoff_percent, -footnotes) |&gt;\n  filter(state == \"WI\") |&gt;\n  filter(district_id == \"03\")\n\nwi_d3\n\n# A tibble: 2 × 10\n  state district_id cand_id   incumbent party primary_votes primary_percent\n  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;lgl&gt;     &lt;chr&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1 WI    03          H6WI03099 TRUE      DEM           33320           0.812\n2 WI    03          H6WI03164 FALSE     DEM            7689           0.187\n# ℹ 3 more variables: general_votes &lt;dbl&gt;, general_percent &lt;dbl&gt;, won &lt;lgl&gt;\n\n\nWisconsin District 3 had 2 democrats running for the house seat.\n\nsrc &lt;- \"http://cdmaps.polisci.ucla.edu/shp/districts113.zip\"\nlcl_zip &lt;- fs::path(tempdir(), \"districts113.zip\")\ndownload.file(src, destfile = lcl_zip)\nlcl_districts &lt;- fs::path(tempdir(), \"districts113\")\nunzip(lcl_zip, exdir = lcl_districts)\ndsn_districts &lt;- fs::path(lcl_districts, \"districtShapes\")\n\ndistricts &lt;- st_read(dsn_districts, layer = \"districts113\") |&gt;\n  mutate(DISTRICT = parse_number(as.character(DISTRICT)))\n\nReading layer `districts113' from data source \n  `/tmp/RtmpeAY0Q8/districts113/districtShapes' using driver `ESRI Shapefile'\nSimple feature collection with 436 features and 15 fields (with 1 geometry empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1473 ymin: 18.91383 xmax: 179.7785 ymax: 71.35256\nGeodetic CRS:  NAD83\n\nwi_shp &lt;- districts |&gt;\n  filter(STATENAME == \"Wisconsin\")\n\n\nwi_merged &lt;- wi_shp |&gt;\n  st_transform(4326) |&gt;\n  inner_join(wi_results, by = c(\"DISTRICT\" = \"district\"))\n\nwi &lt;- ggplot(data = wi_merged, aes(fill = winner)) +\n  annotation_map_tile(zoom = 6, type = \"osm\", progress = \"none\") + \n  geom_sf(alpha = 0.5) +\n  scale_fill_manual(\"Winner\", values = c(\"blue\", \"red\")) + \n  geom_sf_label(aes(label = DISTRICT)) + \n  ggtitle(\"Wisconsin House Winners by District\") +\n  labs(x = element_blank(), y = element_blank())+\n  dark_theme_gray(base_size = 12) + \n  theme(\n        axis.ticks = element_blank(), \n        axis.text = element_blank(), \n        plot.margin = unit(c(0.2, 0.7, 0, 1.5), \"cm\"))\n\nwi\n\n\n\n\n\n\n\nwi_results |&gt;                  \n  select(-state)\n\n# A tibble: 8 × 8\n  district     N total_votes d_votes r_votes other_votes r_prop winner    \n     &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1        1     7      353990  107003  230072       16915  0.650 Republican\n2        2     2      397581  273537  124044           0  0.312 Democrat  \n3        3     2      257401  257401       0           0  0     Democrat  \n4        4     4      285858  220181       0       65677  0     Democrat  \n5        5     3      390507  114477  260706       15324  0.668 Republican\n6        6     4      356935  133072  204147       19716  0.572 Republican\n7        7     4      362061  138643  223418           0  0.617 Republican\n8        8     4      363574  135682  227892           0  0.627 Republican\n\n\nThe map shows the winners of the 2016 House elections in Wisconsin. The district shapes are very irregular. District 4 is extremely small when compared to district 7. There are 2 districts where there isn’t even a republican running. The races where republicans have won the district, they have won by a small margin. However, the margin is not as small as the North Carolina districts that we looked at in class.",
    "crumbs": [
      "Maps: Wisconsin Districts"
    ]
  },
  {
    "objectID": "pages/projects/lstm/index.html",
    "href": "pages/projects/lstm/index.html",
    "title": "ML: Long Short Term Memory on IMDB Reviews",
    "section": "",
    "text": "The goal of this model is to classify whether or not a IMDB review has positive or negative sentiment towards the movie it is reviewing. The Long Short Term Memory Model will be used for binary classification of positive and negative sentiment."
  },
  {
    "objectID": "pages/projects/lstm/index.html#dataset-description",
    "href": "pages/projects/lstm/index.html#dataset-description",
    "title": "ML: Long Short Term Memory on IMDB Reviews",
    "section": "Dataset Description",
    "text": "Dataset Description\nIMDB dataset with 50,000 movie reviews with sentiment intended for binary sentiment classification. The sentiment is either positive or negative. When put through the neural network, sentiment 0 is negative and 1 is positive. The review is raw text data from the IMDB website which needs to be cleaned. The data was uploaded to Kaggle in 2019, IMDB Dataset of 50K Movie Reviews. The data was most than likely scraped from the IMDB Website. Then the data was manually labeled as having positive or negative sentiment."
  },
  {
    "objectID": "pages/simulation.html",
    "href": "pages/simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "We will be using the Airbnb dataset (from data collected by St. Olaf Students in 2016) to simulate prices based on the district in Chicago. We will be using the Central, Northwest, North, and Southwest districts since these districts have the highest amount of listings. We will be filtering out any listings with less than 10 reviews in order to remove any new listings that may not have accurate prices. We will also be filtering out any prices over $300 in order to level the playing field with all districts (the central district has considerably higher price extremes than the other districts).\n\nairbnb &lt;- airbnb_raw |&gt;\n  filter(reviews &gt; 10) |&gt;\n  filter(price &lt; 300) |&gt;\n  filter(district %in% c(\"Southwest\", \"North\", \"Northwest\", \"Central\")) |&gt;\n  \n  mutate(district = case_when(\n    district == \"Southwest\" ~ \"SW\",\n    district == \"North\" ~ \"N\",\n    district == \"Northwest\" ~ \"NW\",\n    district == \"Central\" ~ \"C\"\n  )) |&gt;\n  mutate(district = as.factor(district)) |&gt;\n  select(price, bedrooms, accommodates, district)",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "pages/simulation.html#airbnb-data",
    "href": "pages/simulation.html#airbnb-data",
    "title": "Simulation",
    "section": "",
    "text": "We will be using the Airbnb dataset (from data collected by St. Olaf Students in 2016) to simulate prices based on the district in Chicago. We will be using the Central, Northwest, North, and Southwest districts since these districts have the highest amount of listings. We will be filtering out any listings with less than 10 reviews in order to remove any new listings that may not have accurate prices. We will also be filtering out any prices over $300 in order to level the playing field with all districts (the central district has considerably higher price extremes than the other districts).\n\nairbnb &lt;- airbnb_raw |&gt;\n  filter(reviews &gt; 10) |&gt;\n  filter(price &lt; 300) |&gt;\n  filter(district %in% c(\"Southwest\", \"North\", \"Northwest\", \"Central\")) |&gt;\n  \n  mutate(district = case_when(\n    district == \"Southwest\" ~ \"SW\",\n    district == \"North\" ~ \"N\",\n    district == \"Northwest\" ~ \"NW\",\n    district == \"Central\" ~ \"C\"\n  )) |&gt;\n  mutate(district = as.factor(district)) |&gt;\n  select(price, bedrooms, accommodates, district)",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "pages/simulation.html#exploratory-data-analysis",
    "href": "pages/simulation.html#exploratory-data-analysis",
    "title": "Simulation",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThe prices by bedrooms and accommodates show a positive correlation. However, there does not seem to be any relationship between district and either bedrooms or accommodates. We can see that prices are considerably higher in the Central district compared to the other districts. While the Southwest district has the lowest prices.\n\nggplot(airbnb, aes(x = price, y = bedrooms, color = district)) +\n  geom_point() +\n  labs(\n    title = \"Airbnb Prices by Bedrooms\",\n    x = \"Price\",\n    y = \"Bedrooms\",\n    color = \"District\"\n  ) +\n  dark_theme_gray(base_size = 12)\n\n\n\n\n\n\n\nggplot(airbnb, aes(x = price, y = accommodates, color = district)) +\n  geom_point() +\n  labs(\n    title = \"Airbnb Prices by Accommodates\",\n    x = \"Price\",\n    y = \"Accommodates\",\n    color = \"District\"\n  ) +\n  dark_theme_gray(base_size = 12)\n\n\n\n\n\n\n\nggplot(airbnb, aes(x = district, y = price, fill = district)) +\n  geom_boxplot() +\n  labs(\n    title = \"Airbnb Prices by District\",\n    x = \"District\",\n    y = \"Price\",\n    fill = \"District\"\n  ) +\n  dark_theme_gray(base_size = 12)",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "pages/simulation.html#generating-prices",
    "href": "pages/simulation.html#generating-prices",
    "title": "Simulation",
    "section": "Generating Prices",
    "text": "Generating Prices\nThis function generates prices based on the true mean and standard deviation. It also takes in the districts and the number of listings in each district. This function will be used later to simulate data for the Central, Northwest, North, and Southwest districts.\n\ngen_prices &lt;- function(true_mean, true_sd, districts, num_in_district, id = 1) {\n  table &lt;- tibble(\n    district = factor(),\n    price = numeric(),\n    id = character()\n  )\n\n  for (i in 1:length(districts)) {\n    sample &lt;- rnorm(num_in_district$n[i], mean = true_mean, sd = true_sd)\n    table &lt;- bind_rows(table, tibble(district = rep(districts[i], num_in_district$n[i]), price = sample, id = rep(id, num_in_district$n[i])))\n  }\n\n  return(table)\n}",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "pages/simulation.html#defining-parameters",
    "href": "pages/simulation.html#defining-parameters",
    "title": "Simulation",
    "section": "Defining Parameters",
    "text": "Defining Parameters\n\nprice_mean &lt;- mean(airbnb$price)\nprice_sd &lt;- sd(airbnb$price)\n\nnum_in_district &lt;- airbnb |&gt;\n  group_by(district) |&gt;\n  summarize(n = n())\n\nmean_num_in_district &lt;- mean(num_in_district$n)",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "pages/simulation.html#simulating-data",
    "href": "pages/simulation.html#simulating-data",
    "title": "Simulation",
    "section": "Simulating Data",
    "text": "Simulating Data\n\nn_sims &lt;- 8\nparams &lt;- tibble(\n  sd = c(rep(price_sd, n_sims)),\n  id = c(paste(\"Sim\", 1:n_sims))\n)\n\nsimulation &lt;- params %&gt;%\n  pmap_dfr(~ gen_prices(true_mean = price_mean, true_sd = ..1, districts = c(\"SW\", \"N\", \"NW\", \"C\"), num_in_district = num_in_district, id = ..2))\n\nairbnb_format &lt;- airbnb |&gt;\n  mutate(id = \"Truth\") |&gt;\n  select(district, price, id)\n\nfinal_sim &lt;- rbind(simulation, airbnb_format)\n\nWe can determine from the simulated samples that there is a large difference from the total mean price of Airbnbs in the Chicago areas, specifically on Central and Southwest Chicago. For the North and Northwest regions, they appear to be consistent with the simulated box plots. The higher prices that we see with the central Airbnb’s could be related to its close proximity to downtown Chicago which would have a higher cost of living. Furthermore, Southwest Chicago might have fewer things to do and may not be as desirable for tourism. If we wanted to be sure of our findings, we could do a ANOVA statistical test.\n\nggplot(final_sim, aes(x = district, y = price, fill = district)) +\n  geom_boxplot() +\n  labs(\n    title = \"Simulated Prices by District\",\n    x = \"District\",\n    y = \"Price\",\n    fill = \"District\"\n  ) +\n  scale_y_continuous(limits = c(0, max(final_sim$price) + 100)) +\n  dark_theme_gray(base_size = 12) +\n  facet_wrap(~id)",
    "crumbs": [
      "Simulation"
    ]
  }
]